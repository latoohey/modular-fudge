{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f4c0861-e283-4c40-bad9-16de50eae553",
      "metadata": {
        "id": "9f4c0861-e283-4c40-bad9-16de50eae553"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64cbf53-d971-4563-8268-b44db401c5f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c64cbf53-d971-4563-8268-b44db401c5f6",
        "outputId": "15ac812a-d060-40a5-e619-a80d71685a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8249b979-e6f4-452c-91eb-be9897a27473",
      "metadata": {
        "id": "8249b979-e6f4-452c-91eb-be9897a27473"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict\n",
        "from types import SimpleNamespace\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ef150e-da7f-494b-aaf9-671fb7abb026",
      "metadata": {
        "id": "46ef150e-da7f-494b-aaf9-671fb7abb026"
      },
      "source": [
        "## arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9420de19-d5eb-4f9a-a4a1-e9935f1315a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9420de19-d5eb-4f9a-a4a1-e9935f1315a4",
        "outputId": "0e0a6b38-de6d-4dc5-c276-e48525faa15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On Colab\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ec9aefca150>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "args= SimpleNamespace(\n",
        "    data_dir=\"data\",\n",
        "    save_dir=\"trained_models\",\n",
        "    hf_token=\"XXXXX\",\n",
        "    ckpt=None,\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    lr=1e-4,\n",
        "    seed=24601,\n",
        "    num_workers=0,\n",
        "    pos_cat=\"encyclopedia\",\n",
        "    neg_cat=\"holmes\",\n",
        "    print_freq=100,\n",
        "    model_type=\"lstm\",\n",
        "    lstm_hidden_dim=300,\n",
        "    lstm_num_layers=3,\n",
        ")\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"On Colab\")\n",
        "    from google.colab import drive, userdata\n",
        "    drive.mount('/content/drive')\n",
        "    args.hf_token = userdata.get('HF_TOKEN')\n",
        "    args.data_dir = \"/content/drive/My Drive/modular-fudge/\" + args.data_dir\n",
        "    args.save_dir = \"/content/drive/My Drive/modular-fudge/\" + args.save_dir\n",
        "\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60200097-ede5-45cc-ac3d-fd84023437cb",
      "metadata": {
        "id": "60200097-ede5-45cc-ac3d-fd84023437cb"
      },
      "source": [
        "## constants.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34658b2-481f-4b03-a118-af6d4b2e3ec9",
      "metadata": {
        "id": "c34658b2-481f-4b03-a118-af6d4b2e3ec9"
      },
      "outputs": [],
      "source": [
        "# --- Tokenizer ---\n",
        "TOKENIZER_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "PAD_TOKEN = '[PAD]'\n",
        "\n",
        "# --- Data Processing ---\n",
        "VAL_SIZE = 400\n",
        "MAX_LEN = 200\n",
        "MIN_SENTENCE_LENGTH = 3\n",
        "\n",
        "# --- Training ---\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5354363-ebfa-4f8c-a646-f40eb12d8eed",
      "metadata": {
        "id": "f5354363-ebfa-4f8c-a646-f40eb12d8eed"
      },
      "source": [
        "## util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d79654-70dd-47b7-a22c-b5da7ba70bb7",
      "metadata": {
        "id": "45d79654-70dd-47b7-a22c-b5da7ba70bb7"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, save_path):\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    torch.save(state, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3997efd3-0aff-4b62-9d7e-fba83e422da8",
      "metadata": {
        "id": "3997efd3-0aff-4b62-9d7e-fba83e422da8"
      },
      "outputs": [],
      "source": [
        "def num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee5191c-c09f-43af-b9b8-e15d3bfa6e0c",
      "metadata": {
        "id": "aee5191c-c09f-43af-b9b8-e15d3bfa6e0c"
      },
      "outputs": [],
      "source": [
        "def pad_mask(lengths: torch.LongTensor) -> torch.ByteTensor:\n",
        "    \"\"\"\n",
        "    Create a mask of batch x seq where 1 is for non-padding\n",
        "    and 0 is for padding.\n",
        "    \"\"\"\n",
        "    max_seqlen = torch.max(lengths)\n",
        "    # (max_seqlen, batch_size)\n",
        "    expanded_lengths = lengths.unsqueeze(0).repeat((max_seqlen, 1))\n",
        "    # (max_seqlen, batch_size)\n",
        "    indices = torch.arange(max_seqlen).unsqueeze(1).repeat((1, lengths.size(0))).to(lengths.device)\n",
        "\n",
        "    # (max_seqlen, batch_size) -> (batch_size, max_seqlen)\n",
        "    return (expanded_lengths > indices).permute(1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b15a76e-fba2-4530-885d-643e5b65d3b5",
      "metadata": {
        "id": "9b15a76e-fba2-4530-885d-643e5b65d3b5"
      },
      "outputs": [],
      "source": [
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries.append(time.ctime(time.time()))\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cada0ad-dc60-4486-aa80-8007757c4310",
      "metadata": {
        "id": "2cada0ad-dc60-4486-aa80-8007757c4310"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        if self.count > 0:\n",
        "            self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b657bf93-b952-46c6-baa6-37ed39df4c14",
      "metadata": {
        "id": "b657bf93-b952-46c6-baa6-37ed39df4c14"
      },
      "source": [
        "## data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660383f3-de1a-44e9-b985-d4196ae933c3",
      "metadata": {
        "id": "660383f3-de1a-44e9-b985-d4196ae933c3"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    \"\"\"\n",
        "    This collate function is unchanged from the original,\n",
        "    as it's needed to pad the inputs and handle the labels.\n",
        "    \"\"\"\n",
        "    pad_id = batch[0][2] # Now index 2\n",
        "    inputs = [b[0] for b in batch]\n",
        "    lengths = torch.LongTensor([b[1] for b in batch])\n",
        "    max_length = lengths.max()\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "        if len(inputs[i]) < max_length:\n",
        "            # Pad with 0, as that's the embedding padding_idx\n",
        "            inputs[i] = torch.cat([inputs[i], torch.zeros(max_length - len(inputs[i])).long()], dim=0)\n",
        "\n",
        "    inputs = torch.stack(inputs, dim=0)\n",
        "\n",
        "    # Get the single integer label (index 3)\n",
        "    classification_labels = [b[3] for b in batch]\n",
        "    classification_labels = torch.LongTensor(classification_labels)\n",
        "\n",
        "    return (inputs, lengths, classification_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ba61a7-001c-4aa8-8738-ba08bf1a7ba3",
      "metadata": {
        "id": "b1ba61a7-001c-4aa8-8738-ba08bf1a7ba3"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, args):\n",
        "        print('Loading data...')\n",
        "        self.data_dir = args.data_dir\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "\n",
        "        # Add pad token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})\n",
        "\n",
        "        self.tokenizer_pad_id = self.tokenizer.pad_token_id\n",
        "\n",
        "        train, val, test = [], [], []\n",
        "\n",
        "        # --- Process Train & Val from the 'train' directory ---\n",
        "        for category, label in [(args.pos_cat, 1), (args.neg_cat, 0)]:\n",
        "            train_file_path = os.path.join(args.data_dir, 'splits', 'train', f'{category}.txt')\n",
        "\n",
        "            # Check if file exists before opening\n",
        "            if not os.path.exists(train_file_path):\n",
        "                print(f\"Warning: Train file not found at {train_file_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            with open(train_file_path, 'r', encoding='utf-8') as rf:\n",
        "                for i, line in enumerate(rf):\n",
        "                    # ... (line truncation logic) ...\n",
        "\n",
        "                    if i < VAL_SIZE // 2:\n",
        "                        val.append((line.strip(), label))\n",
        "                    else:\n",
        "                        train.append((line.strip(), label))\n",
        "\n",
        "        # --- Process Test from the 'test' directory ---\n",
        "        for category, label in [(args.pos_cat, 1), (args.neg_cat, 0)]:\n",
        "            # Notice the change from 'train' to 'test' in the path\n",
        "            test_file_path = os.path.join(args.data_dir, 'splits', 'test', f'{category}.txt')\n",
        "\n",
        "            # Check if file exists before opening\n",
        "            if not os.path.exists(test_file_path):\n",
        "                print(f\"Warning: Test file not found at {test_file_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            with open(test_file_path, 'r', encoding='utf-8') as rf:\n",
        "                for line in rf:\n",
        "                    # ... (line truncation logic) ...\n",
        "                    test.append((line.strip(), label))\n",
        "\n",
        "        # This part remains the same\n",
        "        self.splits = {'train': train, 'val': val, 'test': test}\n",
        "        print('Done loading data. Split sizes:')\n",
        "        for key in self.splits:\n",
        "            print(f\"{key}: {len(self.splits[key])}\")\n",
        "\n",
        "    def shuffle(self, split, seed=None):\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "        random.shuffle(self.splits[split])\n",
        "\n",
        "    def loader(self, split, batch_size, num_workers=0, indices=None):\n",
        "        data = self.splits[split] if indices is None else [self.splits[split][i] for i in indices]\n",
        "        return DataLoader(\n",
        "            SplitLoader(data, self),\n",
        "            batch_size=batch_size,\n",
        "            pin_memory=True,\n",
        "            collate_fn=collate,\n",
        "            num_workers=num_workers\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ead21f6-3b92-4804-acee-8ef44615429f",
      "metadata": {
        "id": "6ead21f6-3b92-4804-acee-8ef44615429f"
      },
      "outputs": [],
      "source": [
        "class SplitLoader(IterableDataset):\n",
        "    def __init__(self, data, parent):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.pos = 0\n",
        "        self.parent = parent\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.pos = 0 # Reset for new epoch\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # This logic is simplified from the original multi-threaded worker logic\n",
        "        # for clarity. It will work correctly with num_workers=0.\n",
        "        if self.pos >= len(self):\n",
        "            raise StopIteration\n",
        "\n",
        "        raw_sentence, classification_label = self.data[self.pos]\n",
        "        self.pos += 1\n",
        "\n",
        "        sentence_tokens = self.parent.tokenizer.encode(raw_sentence, return_tensors='pt')[0]\n",
        "        length = len(sentence_tokens)\n",
        "\n",
        "        if length < MIN_SENTENCE_LENGTH:\n",
        "            # Skip this item and try the next one\n",
        "            return self.__next__()\n",
        "\n",
        "        pad_id = self.parent.tokenizer_pad_id\n",
        "\n",
        "        # Return (input_tokens, length, pad_id, label)\n",
        "        # collate fn will handle the pad_id\n",
        "        return (sentence_tokens, length, pad_id, classification_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0717723-a806-42b1-81b4-50de25dd7e15",
      "metadata": {
        "id": "f0717723-a806-42b1-81b4-50de25dd7e15"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a67a5fc-0de3-49d1-a0dd-424dbff15a28",
      "metadata": {
        "id": "8a67a5fc-0de3-49d1-a0dd-424dbff15a28"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f07521-eb3f-4c31-a819-82d9af6b1dfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2f07521-eb3f-4c31-a819-82d9af6b1dfb",
        "outputId": "c802d535-5437-4e4f-f9f5-f2f950c14ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:11: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\m'\n",
            "/tmp/ipython-input-3515582310.py:11: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  (e.g., `models\\my_new_model.py`) and implement the following components:\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "========================================================================\n",
        "Model Definition File Contract\n",
        "========================================================================\n",
        "\n",
        "This file defines a classifier model architecture that is compatible with\n",
        "the project's main training (`main_train.py`) and evaluation (`evaluate.py`)\n",
        "scripts.\n",
        "\n",
        "To add a new model (e.g., \"MyNewModel\"), create a new file like this one\n",
        "(e.g., `models\\my_new_model.py`) and implement the following components:\n",
        "\n",
        "1.  A class that inherits from `torch.nn.Module`.\n",
        "2.  An `__init__` method with a specific signature.\n",
        "3.  An *internal* `forward` method for the model's logic.\n",
        "4.  A `get_scores_for_batch` \"adapter\" method for training.\n",
        "5.  A `get_final_scores` \"adapter\" method for evaluation.\n",
        "\n",
        "The factory in `models\\__init__.py` must also be updated to import\n",
        "and select this new class based on the `--model_type` argument.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "CONTRACT DETAILS\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "--- [1. `__init__` Method] ---\n",
        "\n",
        "The `__init__` method *must* have the following signature:\n",
        "\n",
        "def __init__(self, args, vocab_size):\n",
        "    ...\n",
        "\n",
        "    - `args`: The fully populated `ArgumentParser` namespace. This\n",
        "      object will contain all command-line arguments, allowing the\n",
        "      model to pull its own specific hyperparameters (e.g.,\n",
        "      `args.my_model_hidden_dim`, `args.my_model_num_layers`).\n",
        "\n",
        "    - `vocab_size`: An integer (e.g., from `len(tokenizer)`)\n",
        "      specifying the total vocabulary size. This is required to\n",
        "      correctly initialize the `nn.Embedding` layer.\n",
        "\n",
        "--- [2. `forward` Method] ---\n",
        "\n",
        "The `forward` method is *internal* to your model. Its signature can\n",
        "be whatever you need.\n",
        "\n",
        "    - Example: `def forward(self, inputs, lengths):` (for LSTM)\n",
        "    - Example: `def forward(self, inputs):` (for Mamba/Transformer)\n",
        "\n",
        "This method will contain the core architectural logic (embeddings,\n",
        "RNN/Mamba/Transformer layers, output head).\n",
        "\n",
        "It *must* be causal (unidirectional) and output a tensor of\n",
        "per-token logits.\n",
        "\n",
        "    - **Output Shape:** `(batch_size, seq_len)`\n",
        "\n",
        "--- [3. `get_scores_for_batch` Method] ---\n",
        "\n",
        "This is the adapter method called by `main_train.py`. It is\n",
        "responsible for unpacking the batch, calling its own `forward`\n",
        "method, and returning *all* per-token scores for the loss\n",
        "calculation.\n",
        "\n",
        "    - **Input:** `batch` (The raw, collated batch from the DataLoader.\n",
        "      Typically `[inputs, lengths, classification_targets]`)\n",
        "\n",
        "    - **Returns:** A tuple of `(scores, targets)`\n",
        "        - `scores`: `torch.Tensor` of shape `(batch_size, seq_len)`\n",
        "          (The per-token logits from the `forward` pass).\n",
        "        - `targets`: `torch.Tensor` of shape `(batch_size,)`\n",
        "          (The true class labels, e.g., [0, 1, 1, 0]).\n",
        "\n",
        "--- [4. `get_final_scores` Method] ---\n",
        "\n",
        "This is the adapter method called by `evaluate.py`. It is\n",
        "responsible for unpacking the batch, calling `forward`, and\n",
        "returning the logit from *only* the single, final, unpadded token.\n",
        "\n",
        "    - **Input:** `batch` (The raw, collated batch, same as above).\n",
        "\n",
        "    - **Returns:** `last_logits`\n",
        "        - `last_logits`: `torch.Tensor` of shape `(batch_size,)`\n",
        "          (The logit from the last *real* token for each item\n",
        "          in the batch).\n",
        "\"\"\"\n",
        "def comment_only():\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3e0631-6cf8-4b17-83e2-7ad716992e30",
      "metadata": {
        "id": "5b3e0631-6cf8-4b17-83e2-7ad716992e30"
      },
      "outputs": [],
      "source": [
        "# --- Model Architecture ---\n",
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab_size):\n",
        "        \"\"\"\n",
        "        Initializes the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            args: The full ArgumentParser namespace. Reads\n",
        "                  `args.lstm_hidden_dim` and `args.lstm_num_layers`.\n",
        "            vocab_size: The total vocabulary size for the embedding layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # --- CRITICAL CHANGE ---\n",
        "        # Using `vocab_size` (e.g., 32000) is robust and correct.\n",
        "        # Using `tokenizer_pad_id + 1` was brittle and would fail\n",
        "        # with many tokenizers where the pad ID is not the highest ID.\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=args.lstm_hidden_dim,\n",
        "            padding_idx=0  # Assuming 0 is your pad ID\n",
        "        )\n",
        "        # --- End of Change ---\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            args.lstm_hidden_dim,\n",
        "            args.lstm_hidden_dim,\n",
        "            num_layers=args.lstm_num_layers,\n",
        "            bidirectional=False,\n",
        "            dropout=0.5,\n",
        "            batch_first=True # Makes the permute/transpose logic simpler\n",
        "        )\n",
        "        self.out_linear = nn.Linear(args.lstm_hidden_dim, 1)\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        \"\"\"\n",
        "        Internal forward pass for the LSTM.\n",
        "        Requires `lengths` for sequence packing.\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_len, hidden_dim)\n",
        "        embedded_inputs = self.embed(inputs)\n",
        "\n",
        "        # Pack sequence for efficient RNN processing\n",
        "        packed_inputs = pack_padded_sequence(\n",
        "            embedded_inputs,\n",
        "            lengths.cpu(), # Must be on CPU\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # rnn_output is (packed_batch, hidden_dim)\n",
        "        rnn_output, _ = self.rnn(packed_inputs)\n",
        "\n",
        "        # Unpack: (batch_size, seq_len, hidden_dim)\n",
        "        rnn_output, _ = pad_packed_sequence(\n",
        "            rnn_output,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # (batch_size, seq_len)\n",
        "        return self.out_linear(rnn_output).squeeze(2)\n",
        "\n",
        "    # ---\n",
        "    # --- Adapter Methods (The \"Contract\") ---\n",
        "    # ---\n",
        "\n",
        "    def get_scores_for_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for training.\n",
        "        Unpacks batch, calls `self.forward`, and returns all scores.\n",
        "        \"\"\"\n",
        "        # Unpack the batch as needed *by this model*\n",
        "        inputs, lengths, classification_targets = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "\n",
        "        # Call this model's specific forward pass\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Return what the training loop needs\n",
        "        return scores, classification_targets\n",
        "\n",
        "    def get_final_scores(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for evaluation.\n",
        "        Unpacks batch, calls `self.forward`, and returns final logit.\n",
        "        \"\"\"\n",
        "        # We need all 3 components from the batch\n",
        "        inputs, lengths, _ = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "\n",
        "        # Call this model's specific forward pass\n",
        "        # scores shape: (batch_size, seq_len)\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Find the index of the last token\n",
        "        # Shape: (batch_size,)\n",
        "        last_indices = (lengths - 1).long()\n",
        "\n",
        "        # Gather the specific scores from those last indices\n",
        "        # Shape: (batch_size, 1) -> (batch_size,)\n",
        "        last_logits = scores.gather(\n",
        "            1, last_indices.unsqueeze(1)\n",
        "        ).squeeze(1)\n",
        "\n",
        "        return last_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7044d4c-88b7-448d-95f5-da398cda5551",
      "metadata": {
        "id": "f7044d4c-88b7-448d-95f5-da398cda5551"
      },
      "source": [
        "## model init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbbd131-1784-448f-a7c7-1a47259ae942",
      "metadata": {
        "id": "9bbbd131-1784-448f-a7c7-1a47259ae942"
      },
      "outputs": [],
      "source": [
        "def get_model(args, tokenizer_pad_id):\n",
        "    \"\"\"\n",
        "    This factory function reads the --model_type argument\n",
        "    and returns the correct, initialized model.\n",
        "    \"\"\"\n",
        "    if args.model_type == 'lstm':\n",
        "        return LSTMClassifier(args, tokenizer_pad_id)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {args.model_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77d7f6d-8777-4c0c-af44-b0e2c6db4753",
      "metadata": {
        "id": "b77d7f6d-8777-4c0c-af44-b0e2c6db4753"
      },
      "source": [
        "## main_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5d3826-c3d9-4a3e-a7b5-e7688ce3254c",
      "metadata": {
        "id": "fc5d3826-c3d9-4a3e-a7b5-e7688ce3254c"
      },
      "outputs": [],
      "source": [
        "def train(model, dataset, optimizer, criterion, epoch, args):\n",
        "    model.train()\n",
        "    dataset.shuffle('train', seed=epoch + args.seed)\n",
        "\n",
        "    loader = dataset.loader('train', args.batch_size, num_workers=args.num_workers)\n",
        "    loss_meter = AverageMeter('loss', ':6.4f')\n",
        "    total_length = len(loader)\n",
        "    progress = ProgressMeter(total_length, [loss_meter], prefix='Training: ')\n",
        "\n",
        "    for batch_num, batch in enumerate(tqdm(loader, total=len(loader))):\n",
        "        # Unpack the simplified batch\n",
        "        inputs, lengths, classification_targets = batch\n",
        "\n",
        "        # Move to device BEFORE calling model's forward pass\n",
        "        inputs = inputs.to(args.device)\n",
        "        lengths = lengths.to(args.device)\n",
        "        classification_targets = classification_targets.to(args.device)\n",
        "\n",
        "        # Get per-token scores from the model\n",
        "        scores = model(inputs, lengths) # (batch_size, seq_len)\n",
        "\n",
        "        # --- This is the \"Implicit Prefix\" Loss Logic ---\n",
        "\n",
        "        expanded_labels = classification_targets.unsqueeze(1).expand(-1, scores.shape[1])\n",
        "\n",
        "        # 2. Get padding mask (batch_size, seq_len)\n",
        "        length_mask = pad_mask(lengths) # 1 for real tokens, 0 for padding\n",
        "\n",
        "        # 3. Flatten scores, labels, and mask\n",
        "        scores_flat = scores.flatten()\n",
        "        labels_flat = expanded_labels.flatten().float()\n",
        "        mask_flat = length_mask.flatten()\n",
        "\n",
        "        # 4. Select only the non-padded tokens for loss calculation\n",
        "        scores_unpadded = scores_flat[mask_flat == 1]\n",
        "        labels_unpadded = labels_flat[mask_flat == 1]\n",
        "\n",
        "        # 5. Calculate loss\n",
        "        loss = criterion(scores_unpadded, labels_unpadded)\n",
        "        # --- End of Implicit Logic ---\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_meter.update(loss.item(), inputs.size(0))\n",
        "        if batch_num % args.print_freq == 0:\n",
        "            progress.display(batch_num)\n",
        "\n",
        "    progress.display(total_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a22a959a-4ca4-4fdc-8eda-59b9eaae04ff",
      "metadata": {
        "id": "a22a959a-4ca4-4fdc-8eda-59b9eaae04ff"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataset, criterion, args):\n",
        "    model.eval()\n",
        "    loader = dataset.loader('val', args.batch_size, num_workers=args.num_workers)\n",
        "    loss_meter = AverageMeter('loss', ':6.4f')\n",
        "    total_length = len(loader)\n",
        "    progress = ProgressMeter(total_length, [loss_meter], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, batch in enumerate(tqdm(loader, total=len(loader))):\n",
        "            inputs, lengths, classification_targets = batch\n",
        "            # Move to device\n",
        "            inputs = inputs.to(args.device)\n",
        "            lengths = lengths.to(args.device)\n",
        "            classification_targets = classification_targets.to(args.device)\n",
        "\n",
        "            scores = model(inputs, lengths)\n",
        "\n",
        "            # --- Identical Implicit Loss Logic ---\n",
        "            expanded_labels = classification_targets.unsqueeze(1).expand(-1, scores.shape[1])\n",
        "            length_mask = pad_mask(lengths)\n",
        "\n",
        "            scores_flat = scores.flatten()\n",
        "            labels_flat = expanded_labels.flatten().float()\n",
        "            mask_flat = length_mask.flatten()\n",
        "\n",
        "            scores_unpadded = scores_flat[mask_flat == 1]\n",
        "            labels_unpadded = labels_flat[mask_flat == 1]\n",
        "\n",
        "            if scores_unpadded.nelement() > 0: # Avoid empty batches if all are filtered\n",
        "                loss = criterion(scores_unpadded, labels_unpadded)\n",
        "                loss_meter.update(loss.item(), inputs.size(0))\n",
        "\n",
        "    progress.display(total_length)\n",
        "    return loss_meter.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c252e0-752d-4389-90fc-1a37ec0f1f1b",
      "metadata": {
        "id": "40c252e0-752d-4389-90fc-1a37ec0f1f1b"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    login(token=args.hf_token)\n",
        "\n",
        "    # Hard-code the task\n",
        "    args.task = 'transfer'\n",
        "    args.device = torch.device(DEVICE)\n",
        "\n",
        "    dataset = Dataset(args)\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "\n",
        "    model = get_model(args, dataset.tokenizer_pad_id)\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    best_val_metric = 1e8 # Lower is better for BCE\n",
        "\n",
        "    print('Model Parameters:', num_params(model))\n",
        "    criterion = nn.BCEWithLogitsLoss().to(args.device)\n",
        "\n",
        "    now = datetime.now()\n",
        "    date_string = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"--- TRAINING: Epoch {epoch} at {time.ctime()} ---\")\n",
        "        train(model, dataset, optimizer, criterion, epoch, args)\n",
        "\n",
        "        print(f\"--- VALIDATION: Epoch {epoch} at {time.ctime()} ---\")\n",
        "        metric = validate(model, dataset, criterion, args)\n",
        "\n",
        "        if metric < best_val_metric:\n",
        "            print(f'New best val metric: {metric:.4f}')\n",
        "            best_val_metric = metric\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_metric': best_val_metric,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'args': args\n",
        "            }, os.path.join(args.save_dir, f'{args.model_type}_{date_string}.pth.tar'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4484f0c6-ddce-40c1-a3e9-7ad98e47a01e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4484f0c6-ddce-40c1-a3e9-7ad98e47a01e",
        "outputId": "38a967e5-12c5-43b3-b1ec-7c434b384bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Done loading data. Split sizes:\n",
            "train: 2000\n",
            "val: 400\n",
            "test: 600\n",
            "Model Parameters: 40644301\n",
            "--- TRAINING: Epoch 0 at Sun Nov  9 22:26:27 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 2/32 [00:01<00:21,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:26:28 2025\tloss 0.6914 (0.6914)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:07<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:26:34 2025\tloss 0.6520 (0.6818)\n",
            "--- VALIDATION: Epoch 0 at Sun Nov  9 22:26:34 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:26:35 2025\tloss 0.7349 (0.6387)\n",
            "New best val metric: 0.6387\n",
            "--- TRAINING: Epoch 1 at Sun Nov  9 22:26:36 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  5.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:26:37 2025\tloss 0.6392 (0.6392)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:26:42 2025\tloss 0.1347 (0.3921)\n",
            "--- VALIDATION: Epoch 1 at Sun Nov  9 22:26:42 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:26:43 2025\tloss 0.0659 (0.1186)\n",
            "New best val metric: 0.1186\n",
            "--- TRAINING: Epoch 2 at Sun Nov  9 22:26:45 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:26:45 2025\tloss 0.1222 (0.1222)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:26:51 2025\tloss 0.0529 (0.1363)\n",
            "--- VALIDATION: Epoch 2 at Sun Nov  9 22:26:51 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:26:52 2025\tloss 0.0744 (0.1088)\n",
            "New best val metric: 0.1088\n",
            "--- TRAINING: Epoch 3 at Sun Nov  9 22:26:53 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:26:53 2025\tloss 0.0977 (0.0977)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:26:59 2025\tloss 0.1110 (0.0949)\n",
            "--- VALIDATION: Epoch 3 at Sun Nov  9 22:26:59 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:00 2025\tloss 0.0374 (0.0752)\n",
            "New best val metric: 0.0752\n",
            "--- TRAINING: Epoch 4 at Sun Nov  9 22:27:01 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 2/32 [00:00<00:05,  5.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:01 2025\tloss 0.0556 (0.0556)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:07 2025\tloss 0.2308 (0.0911)\n",
            "--- VALIDATION: Epoch 4 at Sun Nov  9 22:27:07 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:08 2025\tloss 0.0467 (0.1114)\n",
            "--- TRAINING: Epoch 5 at Sun Nov  9 22:27:08 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:08 2025\tloss 0.1186 (0.1186)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:15 2025\tloss 0.0715 (0.1078)\n",
            "--- VALIDATION: Epoch 5 at Sun Nov  9 22:27:15 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:16 2025\tloss 0.0374 (0.0842)\n",
            "--- TRAINING: Epoch 6 at Sun Nov  9 22:27:16 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:16 2025\tloss 0.0740 (0.0740)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:22 2025\tloss 0.0361 (0.0744)\n",
            "--- VALIDATION: Epoch 6 at Sun Nov  9 22:27:22 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:23 2025\tloss 0.0375 (0.0753)\n",
            "--- TRAINING: Epoch 7 at Sun Nov  9 22:27:23 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:23 2025\tloss 0.0629 (0.0629)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:29 2025\tloss 0.0320 (0.0567)\n",
            "--- VALIDATION: Epoch 7 at Sun Nov  9 22:27:29 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:30 2025\tloss 0.0256 (0.0893)\n",
            "--- TRAINING: Epoch 8 at Sun Nov  9 22:27:30 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:30 2025\tloss 0.0273 (0.0273)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:36 2025\tloss 0.0335 (0.0520)\n",
            "--- VALIDATION: Epoch 8 at Sun Nov  9 22:27:36 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:37 2025\tloss 0.0315 (0.0964)\n",
            "--- TRAINING: Epoch 9 at Sun Nov  9 22:27:37 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:37 2025\tloss 0.0814 (0.0814)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:43 2025\tloss 0.0393 (0.0483)\n",
            "--- VALIDATION: Epoch 9 at Sun Nov  9 22:27:43 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:44 2025\tloss 0.0709 (0.1279)\n",
            "--- TRAINING: Epoch 10 at Sun Nov  9 22:27:44 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  5.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:44 2025\tloss 0.1337 (0.1337)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:50 2025\tloss 0.0228 (0.0719)\n",
            "--- VALIDATION: Epoch 10 at Sun Nov  9 22:27:50 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:51 2025\tloss 0.0263 (0.1173)\n",
            "--- TRAINING: Epoch 11 at Sun Nov  9 22:27:51 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 2/32 [00:00<00:05,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:51 2025\tloss 0.0424 (0.0424)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:27:57 2025\tloss 0.0507 (0.0433)\n",
            "--- VALIDATION: Epoch 11 at Sun Nov  9 22:27:57 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:27:58 2025\tloss 0.0241 (0.0605)\n",
            "New best val metric: 0.0605\n",
            "--- TRAINING: Epoch 12 at Sun Nov  9 22:27:59 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:27:59 2025\tloss 0.0372 (0.0372)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▋         | 2/32 [00:00<00:05,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:05 2025\tloss 0.0249 (0.0427)\n",
            "--- VALIDATION: Epoch 12 at Sun Nov  9 22:28:05 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:06 2025\tloss 0.0264 (0.0559)\n",
            "New best val metric: 0.0559\n",
            "--- TRAINING: Epoch 13 at Sun Nov  9 22:28:07 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:28:07 2025\tloss 0.0256 (0.0256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:13 2025\tloss 0.0252 (0.0785)\n",
            "--- VALIDATION: Epoch 13 at Sun Nov  9 22:28:13 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:14 2025\tloss 0.0141 (0.1095)\n",
            "--- TRAINING: Epoch 14 at Sun Nov  9 22:28:14 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:28:14 2025\tloss 0.0431 (0.0431)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:20 2025\tloss 0.0275 (0.0502)\n",
            "--- VALIDATION: Epoch 14 at Sun Nov  9 22:28:20 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:21 2025\tloss 0.0199 (0.0564)\n",
            "--- TRAINING: Epoch 15 at Sun Nov  9 22:28:21 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:28:21 2025\tloss 0.0582 (0.0582)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:27 2025\tloss 0.0205 (0.0385)\n",
            "--- VALIDATION: Epoch 15 at Sun Nov  9 22:28:27 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:28 2025\tloss 0.0191 (0.0953)\n",
            "--- TRAINING: Epoch 16 at Sun Nov  9 22:28:28 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:28:29 2025\tloss 0.0369 (0.0369)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:35 2025\tloss 0.0196 (0.0359)\n",
            "--- VALIDATION: Epoch 16 at Sun Nov  9 22:28:35 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:35 2025\tloss 0.0217 (0.0603)\n",
            "--- TRAINING: Epoch 17 at Sun Nov  9 22:28:35 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:06,  5.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:28:36 2025\tloss 0.0241 (0.0241)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:42 2025\tloss 0.0192 (0.0320)\n",
            "--- VALIDATION: Epoch 17 at Sun Nov  9 22:28:42 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:42 2025\tloss 0.0181 (0.0955)\n",
            "--- TRAINING: Epoch 18 at Sun Nov  9 22:28:42 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:28:43 2025\tloss 0.0516 (0.0516)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:49 2025\tloss 0.0170 (0.0334)\n",
            "--- VALIDATION: Epoch 18 at Sun Nov  9 22:28:49 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:49 2025\tloss 0.0218 (0.0857)\n",
            "--- TRAINING: Epoch 19 at Sun Nov  9 22:28:49 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/32 [00:00<00:05,  5.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [ 0/32]\tSun Nov  9 22:28:50 2025\tloss 0.0314 (0.0314)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: [32/32]\tSun Nov  9 22:28:56 2025\tloss 0.1467 (0.0318)\n",
            "--- VALIDATION: Epoch 19 at Sun Nov  9 22:28:56 2025 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00,  7.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [7/7]\tSun Nov  9 22:28:56 2025\tloss 0.0181 (0.1334)\n"
          ]
        }
      ],
      "source": [
        "main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e83c8f7b-8884-4b73-9bfd-cdd9cb548982",
      "metadata": {
        "id": "e83c8f7b-8884-4b73-9bfd-cdd9cb548982"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}