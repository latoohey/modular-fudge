{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8SnjqAAoS0g"
      },
      "source": [
        "# Causal Classifier - Guided Generation (Testing)\n",
        "\n",
        "This notebook is used to run guided text generation using a trained causal classifier from the modular FUDGE project. It loads a base LLM and a trained classifier checkpoint to guide the output in real-time. For a complete overview of the project architecture please see the [full project on the GitHub repo](https://github.com/latoohey/modular-fudge). The training script to create a classifier for guiding generation is also available as a [Colab notebook](https://colab.research.google.com/drive/1zVfBB_zIKHpSANBmTcj8KRBFwsPgbd9m?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7wPSEbfesLd"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p292zU-3PPc2"
      },
      "outputs": [],
      "source": [
        "# For debugging as needed\n",
        "# import os\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LApwcli2o1V3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBf3Ztbko7Qe"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import csv\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from IPython.display import HTML, display\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from functools import lru_cache\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XC_GwG5zqfjZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the directory on your Drive to store the wheels\n",
        "#    We use a specific folder name to keep it organized.\n",
        "wheel_dir = '/content/drive/MyDrive/colab_wheels/mamba_builds'\n",
        "os.makedirs(wheel_dir, exist_ok=True)\n",
        "\n",
        "# 3. Define the package versions you want\n",
        "packages = [\n",
        "    \"causal-conv1d>=1.4.0\",\n",
        "    \"mamba-ssm\"\n",
        "]\n",
        "\n",
        "# 4. Check if wheels already exist in your Drive\n",
        "print(f\"Checking for existing wheels in {wheel_dir}...\")\n",
        "existing_wheels = [f for f in os.listdir(wheel_dir) if f.endswith('.whl')]\n",
        "\n",
        "if len(existing_wheels) >= len(packages):\n",
        "    print(\"‚úÖ Found pre-built wheels! Installing from Drive...\")\n",
        "    # Install directly from your Drive folder\n",
        "    !pip install \"$wheel_dir\"/*.whl\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No wheels found. Building from source (this will take time once)...\")\n",
        "\n",
        "    # Install build dependencies first\n",
        "    !pip install packaging ninja\n",
        "\n",
        "    # Build the wheels and save them directly to your Drive\n",
        "    # We use --no-deps to avoid building wheels for huge packages like PyTorch\n",
        "    print(f\"Building wheels to {wheel_dir}...\")\n",
        "    !pip wheel {\" \".join(packages)} --wheel-dir=\"$wheel_dir\" --no-deps\n",
        "\n",
        "    # Now install the newly built wheels\n",
        "    print(\"Installing newly built wheels...\")\n",
        "    !pip install \"$wheel_dir\"/*.whl\n",
        "\n",
        "print(\"üéâ Done! Mamba and Causal-Conv1d are ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p44DJ7k9Z0Cj"
      },
      "outputs": [],
      "source": [
        "# The Mamba related packages are slow to build\n",
        "# Leave these installs commented unless you intend to test a Mamba model\n",
        "#\n",
        "from mamba_ssm import Mamba\n",
        "import einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOfiHY_3W-_Z"
      },
      "source": [
        "## Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl7FnEDiXYdu"
      },
      "outputs": [],
      "source": [
        "# --- From util.py ---\n",
        "def num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGZwvmwho-d-"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab_size, pad_token_id):\n",
        "        \"\"\"\n",
        "        Initializes the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            args: The full ArgumentParser namespace. Reads\n",
        "                  `args.lstm_hidden_dim` and `args.lstm_num_layers`.\n",
        "            vocab_size: The total vocabulary size for the embedding layer.\n",
        "            pad_token_id: The ID of the padding token.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=args.lstm_hidden_dim,\n",
        "            padding_idx=pad_token_id\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            args.lstm_hidden_dim,\n",
        "            args.lstm_hidden_dim,\n",
        "            num_layers=args.lstm_num_layers,\n",
        "            bidirectional=False,\n",
        "            dropout=0.5,\n",
        "            batch_first=True # Makes the permute/transpose logic simpler\n",
        "        )\n",
        "        self.out_linear = nn.Linear(args.lstm_hidden_dim, 1)\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        \"\"\"\n",
        "        Internal forward pass for the LSTM.\n",
        "        Requires `lengths` for sequence packing.\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_len, hidden_dim)\n",
        "        embedded_inputs = self.embed(inputs)\n",
        "\n",
        "        # Pack sequence for efficient RNN processing\n",
        "        packed_inputs = pack_padded_sequence(\n",
        "            embedded_inputs,\n",
        "            lengths.cpu(), # Must be on CPU\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # rnn_output is (packed_batch, hidden_dim)\n",
        "        rnn_output, _ = self.rnn(packed_inputs)\n",
        "\n",
        "        # Unpack: (batch_size, seq_len, hidden_dim)\n",
        "        rnn_output, _ = pad_packed_sequence(\n",
        "            rnn_output,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # (batch_size, seq_len)\n",
        "        return self.out_linear(rnn_output).squeeze(2)\n",
        "\n",
        "    def get_final_scores(self, batch):\n",
        "        \"\"\"\n",
        "        Returns the scores for the last token of each sequence in the batch.\n",
        "        Used by the guided generation to condition on the last generated token.\n",
        "        \"\"\"\n",
        "        inputs, lengths, _ = batch # _ is targets, not used here\n",
        "        # The forward method returns (batch_size, seq_len)\n",
        "        all_token_scores = self.forward(inputs, lengths)\n",
        "        # Extract the score for the last token of each sequence\n",
        "        # lengths is (batch_size,), all_token_scores is (batch_size, seq_len)\n",
        "        final_scores = all_token_scores[torch.arange(inputs.size(0)), lengths - 1]\n",
        "        return final_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhPe-suFW8GW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "========================================================================\n",
        "Mamba Classifier Model Definition\n",
        "========================================================================\n",
        "\n",
        "This file implements a Mamba-based classifier that follows the same\n",
        "contract as the LSTM classifier for compatibility with the project's\n",
        "main training (`main_train.py`) and evaluation (`evaluate.py`) scripts.\n",
        "\n",
        "The Mamba architecture uses selective state space models (SSMs) for\n",
        "efficient sequence modeling with linear complexity in sequence length.\n",
        "\n",
        "Requirements:\n",
        "- mamba-ssm (install with: pip install mamba-ssm)\n",
        "- torch\n",
        "- einops\n",
        "\"\"\"\n",
        "\n",
        "class MambaClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab_size, pad_token_id):\n",
        "        \"\"\"\n",
        "        Initializes the Mamba model.\n",
        "\n",
        "        Args:\n",
        "            args: The full ArgumentParser namespace. Reads:\n",
        "                  - `args.mamba_d_model` (hidden dimension, default 256)\n",
        "                  - `args.mamba_d_state` (SSM state dimension, default 16)\n",
        "                  - `args.mamba_d_conv` (local convolution width, default 4)\n",
        "                  - `args.mamba_expand` (expansion factor, default 2)\n",
        "                  - `args.mamba_num_layers` (number of Mamba blocks, default 4)\n",
        "                  - `args.mamba_dropout` (dropout rate, default 0.1)\n",
        "            vocab_size: The total vocabulary size for the embedding layer.\n",
        "            pad_token_id: The ID of the padding token.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Get hyperparameters from args with defaults\n",
        "        self.d_model = getattr(args, 'mamba_d_model', 256)\n",
        "        self.d_state = getattr(args, 'mamba_d_state', 16)\n",
        "        self.d_conv = getattr(args, 'mamba_d_conv', 4)\n",
        "        self.expand = getattr(args, 'mamba_expand', 2)\n",
        "        self.num_layers = getattr(args, 'mamba_num_layers', 4)\n",
        "        self.dropout_rate = getattr(args, 'mamba_dropout', 0.1)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.d_model,\n",
        "            padding_idx=pad_token_id  # Use pad_token_id from tokenizer\n",
        "        )\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "        # Stack of Mamba blocks\n",
        "        self.mamba_blocks = nn.ModuleList([\n",
        "            Mamba(\n",
        "                d_model=self.d_model,    # Model dimension\n",
        "                d_state=self.d_state,    # SSM state expansion factor\n",
        "                d_conv=self.d_conv,      # Local convolution width\n",
        "                expand=self.expand,      # Block expansion factor\n",
        "            )\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization between blocks\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(self.d_model)\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm before output\n",
        "        self.final_norm = nn.LayerNorm(self.d_model)\n",
        "\n",
        "        # Output projection to single logit per token\n",
        "        self.out_linear = nn.Linear(self.d_model, 1)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize embedding layer\n",
        "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        # Initialize linear output layer\n",
        "        nn.init.normal_(self.out_linear.weight, mean=0.0, std=0.02)\n",
        "        if self.out_linear.bias is not None:\n",
        "            nn.init.constant_(self.out_linear.bias, 0)\n",
        "\n",
        "    def forward(self, inputs, lengths=None):\n",
        "        \"\"\"\n",
        "        Internal forward pass for the Mamba model.\n",
        "\n",
        "        Note: Mamba handles variable-length sequences naturally without\n",
        "        packing/unpacking, but we accept lengths for compatibility.\n",
        "\n",
        "        Args:\n",
        "            inputs: Token IDs of shape (batch_size, seq_len)\n",
        "            lengths: Sequence lengths (optional, for compatibility)\n",
        "\n",
        "        Returns:\n",
        "            scores: Per-token logits of shape (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.shape\n",
        "\n",
        "        # Embed tokens: (batch_size, seq_len, d_model)\n",
        "        x = self.embed(inputs)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Create causal mask if needed (for padding)\n",
        "        # Mamba is inherently causal, but we need to handle padding\n",
        "        if lengths is not None:\n",
        "            # Create attention mask for padded positions\n",
        "            # Shape: (batch_size, seq_len)\n",
        "            mask = torch.arange(seq_len, device=inputs.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
        "            # Expand mask to match hidden dimension for masking\n",
        "            # Shape: (batch_size, seq_len, 1)\n",
        "            mask = mask.unsqueeze(-1).float()\n",
        "        else:\n",
        "            mask = None\n",
        "\n",
        "        # Process through Mamba blocks with residual connections\n",
        "        for i, (mamba_block, layer_norm) in enumerate(zip(self.mamba_blocks, self.layer_norms)):\n",
        "            # Pre-norm architecture\n",
        "            residual = x\n",
        "            x = layer_norm(x)\n",
        "\n",
        "            # Mamba block\n",
        "            x = mamba_block(x)\n",
        "\n",
        "            # Apply mask if available (zero out padded positions)\n",
        "            if mask is not None:\n",
        "                x = x * mask\n",
        "\n",
        "            # Residual connection and dropout\n",
        "            x = residual + self.dropout(x)\n",
        "\n",
        "        # Final normalization\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Project to logits: (batch_size, seq_len, 1) -> (batch_size, seq_len)\n",
        "        scores = self.out_linear(x).squeeze(-1)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def get_final_scores(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for evaluation.\n",
        "        Unpacks batch, calls `self.forward`, and returns final logit.\n",
        "\n",
        "        Args:\n",
        "            batch: The raw, collated batch from the DataLoader.\n",
        "\n",
        "        Returns:\n",
        "            last_logits: torch.Tensor of shape (batch_size,)\n",
        "                        The logit from the last real token for each item.\n",
        "        \"\"\"\n",
        "        # Unpack the batch\n",
        "        inputs, lengths, _ = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "\n",
        "        # Call forward pass\n",
        "        # scores shape: (batch_size, seq_len)\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Find the index of the last token for each sequence\n",
        "        # Shape: (batch_size,)\n",
        "        last_indices = (lengths - 1).long()\n",
        "\n",
        "        # Gather the scores from the last valid position\n",
        "        # Shape: (batch_size, 1) -> (batch_size,)\n",
        "        last_logits = scores.gather(\n",
        "            1, last_indices.unsqueeze(1)\n",
        "        ).squeeze(1)\n",
        "\n",
        "        return last_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB8_h0QWW8Rt"
      },
      "outputs": [],
      "source": [
        "def get_model(args, vocab_size_param, pad_token_id_param):\n",
        "    \"\"\"\n",
        "    This factory function reads the --model_type argument\n",
        "    and returns the correct, initialized model.\n",
        "    \"\"\"\n",
        "    if args.model_type == 'lstm':\n",
        "        return LSTMClassifier(args, vocab_size_param, pad_token_id_param)\n",
        "    elif args.model_type == 'mamba':\n",
        "        return MambaClassifier(args, vocab_size_param, pad_token_id_param)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {args.model_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxdjt-q1pC8u"
      },
      "outputs": [],
      "source": [
        "# 1. Add the decorator. maxsize=1 is usually enough if you just\n",
        "#    want to hold the current model in memory.\n",
        "@lru_cache(maxsize=1)\n",
        "def load_classifier(ckpt_path, device):\n",
        "    \"\"\"Loads a trained classifier from a checkpoint using the model factory.\"\"\"\n",
        "\n",
        "    # This print statement will only run the FIRST time you call the function\n",
        "    # with a specific path/device combination.\n",
        "    print(f\"Loading classifier from {ckpt_path}...\")\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Load args *from the checkpoint* to know what model to build\n",
        "    model_args = checkpoint['args']\n",
        "    print(f\"Checkpoint args: {model_args}\")\n",
        "\n",
        "    # This assumes your main_train.py saved 'tokenizer_name' in its args\n",
        "    if not hasattr(model_args, 'tokenizer_name'):\n",
        "        tokenizer_name = CLASSIFIER_TOKENIZER_NAME # Ensure this global is defined or passed in\n",
        "    else:\n",
        "        tokenizer_name = model_args.tokenizer_name\n",
        "\n",
        "    classifier_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "            # Add pad token if it doesn't exist\n",
        "    if classifier_tokenizer.pad_token is None:\n",
        "        # 1. Check for Llama 3 specific fine-tune token\n",
        "        if '<|finetune_right_pad_id|>' in classifier_tokenizer.get_vocab():\n",
        "            classifier_tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
        "\n",
        "        # 2. Check for generic reserved tokens (common in TikToken)\n",
        "        elif '<|reserved_special_token_0|>' in classifier_tokenizer.get_vocab():\n",
        "            classifier_tokenizer.pad_token = '<|reserved_special_token_0|>'\n",
        "\n",
        "        # 3. Safe Fallback: Use EOS token (No resizing required)\n",
        "        else:\n",
        "            print(\"Warning: No dedicated pad token found. Using EOS token as PAD.\")\n",
        "            classifier_tokenizer.pad_token = classifier_tokenizer.eos_token\n",
        "\n",
        "    vocab_size = len(classifier_tokenizer)\n",
        "    print(f\"Classifier vocab size: {vocab_size}\")\n",
        "    pad_token_id = classifier_tokenizer.pad_token_id\n",
        "\n",
        "    # --- Use the factory to build the correct model ---\n",
        "    # Ensure get_model is imported or defined in this scope\n",
        "    model = get_model(model_args, vocab_size, pad_token_id)\n",
        "\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Classifier loaded (Type: {model_args.model_type}, Epochs: {checkpoint['epoch']}).\")\n",
        "\n",
        "    # Returns the tuple. The cache will store this entire tuple.\n",
        "    return model, classifier_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1md5jU1WkNr1"
      },
      "outputs": [],
      "source": [
        "def calculate_combined_scores(top_logits, last_token_logits, condition_lambda, use_z_score=False):\n",
        "    \"\"\"\n",
        "    Normalizes and combines LLM logits with Classifier scores.\n",
        "    Returns: combined_log_probs (for selection), final_classifier_scores (for logging), llm_log_probs\n",
        "    \"\"\"\n",
        "    # 1. Normalize LLM scores to log probs\n",
        "    llm_log_probs = F.log_softmax(top_logits, dim=-1)\n",
        "\n",
        "    # --- CHANGE 1: EARLY EXIT FOR OPTIMIZATION ---\n",
        "    # If the classifier was skipped (lambda=0), return pure LLM scores immediately.\n",
        "    if last_token_logits is None:\n",
        "        # Create dummy zeros for the \"classifier scores\" so the logger doesn't crash.\n",
        "        # We make it match the shape of top_logits [1, top_k]\n",
        "        dummy_classifier_scores = torch.zeros_like(top_logits)\n",
        "\n",
        "        # Return: (Pure LLM Scores, Dummy Zeros, Pure LLM Scores)\n",
        "        return llm_log_probs, dummy_classifier_scores, llm_log_probs\n",
        "\n",
        "    # 2. Normalize Classifier scores to log probs\n",
        "    classifier_log_probs = F.log_softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # Extract the \"True\" class score (assuming binary classification index 1 is target)\n",
        "    if len(classifier_log_probs.shape) > 1 and classifier_log_probs.shape[-1] > 1:\n",
        "        relevant_classifier_scores = classifier_log_probs[:, 1]\n",
        "    else:\n",
        "        relevant_classifier_scores = classifier_log_probs\n",
        "\n",
        "    # 3. Apply Strategy\n",
        "    if use_z_score:\n",
        "        # Calculate stats across the top_k candidates\n",
        "        c_mean = relevant_classifier_scores.mean()\n",
        "\n",
        "        # --- CHANGE 2: FIX THE STD() CRASH ---\n",
        "        # unbiased=False prevents crash when top_k=1 (div by zero error)\n",
        "        c_std = relevant_classifier_scores.std(unbiased=False)\n",
        "\n",
        "        if c_std < 1e-8: c_std = 1.0 # Safety\n",
        "\n",
        "        final_classifier_scores = (relevant_classifier_scores - c_mean) / c_std\n",
        "    else:\n",
        "        final_classifier_scores = relevant_classifier_scores\n",
        "\n",
        "    # 4. Combine: LLM_Log_Prob + (Lambda * Classifier_Score)\n",
        "    combined_log_probs = llm_log_probs + (condition_lambda * final_classifier_scores)\n",
        "\n",
        "    return combined_log_probs, final_classifier_scores, llm_log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcPwJLgCTx7l"
      },
      "outputs": [],
      "source": [
        "def select_next_token(combined_log_probs, top_indices, strategy=\"greedy\", temperature=1.0):\n",
        "    \"\"\"\n",
        "    Selects the next token index based on strategy.\n",
        "    Returns: next_token_id (tensor), best_index_relative (int index of top_k)\n",
        "    \"\"\"\n",
        "    if strategy == \"sample\":\n",
        "        # Divide by temp to control randomness\n",
        "        probs = F.softmax(combined_log_probs / temperature, dim=-1)\n",
        "        # Sample from the distribution\n",
        "        best_index_relative = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "        # Greedy default (Returns a 0-dim scalar tensor)\n",
        "        best_index_relative = torch.argmax(combined_log_probs)\n",
        "\n",
        "    # --- THE FIX IS HERE ---\n",
        "    # We force convert to Python int regardless of dimensions.\n",
        "    # argmax returns 0-dim, multinomial returns 2-dim. .item() handles both.\n",
        "    if isinstance(best_index_relative, torch.Tensor):\n",
        "        best_index_relative = int(best_index_relative.item())\n",
        "\n",
        "    # Extract the actual token ID from the top_k list\n",
        "    next_token_id = top_indices[0, best_index_relative].unsqueeze(0)\n",
        "\n",
        "    return next_token_id, best_index_relative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJNtN0PGT2_T"
      },
      "outputs": [],
      "source": [
        "def record_evaluation(evaluation_history, step, generated_ids, tokenizer,\n",
        "                      top_indices, llm_scores, clf_scores, combined_scores, selected_idx):\n",
        "    \"\"\"\n",
        "    Logs the step details to the history list.\n",
        "    \"\"\"\n",
        "    current_context_str = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    top_k = top_indices.shape[1]\n",
        "\n",
        "    step_data = {\n",
        "        \"step\": step,\n",
        "        \"context\": current_context_str,\n",
        "        \"candidates\": []\n",
        "    }\n",
        "\n",
        "    # FIX: Normalize clf_scores to be 1D so we can loop over it easily\n",
        "    # If it came from zeros_like(top_logits), it's [1, K]. We want [K].\n",
        "    if clf_scores.dim() > 1:\n",
        "        clf_scores = clf_scores.squeeze(0)\n",
        "\n",
        "    for i in range(top_k):\n",
        "        cand_id = top_indices[0, i].item()\n",
        "        cand_token = tokenizer.decode([cand_id])\n",
        "\n",
        "        # Safe extraction of scalar values\n",
        "        s_llm = llm_scores[0, i].item()\n",
        "\n",
        "        # FIX: Now we can safely use [i] for both Normal and Optimized cases\n",
        "        s_clf = clf_scores[i].item()\n",
        "\n",
        "        s_comb = combined_scores[0, i].item()\n",
        "        is_winner = (i == selected_idx)\n",
        "\n",
        "        step_data[\"candidates\"].append({\n",
        "            \"token_text\": cand_token,\n",
        "            \"llm_score\": round(s_llm, 4),\n",
        "            \"classifier_score\": round(s_clf, 4),\n",
        "            \"weighted_combined\": round(s_comb, 4),\n",
        "            \"selected\": is_winner\n",
        "        })\n",
        "\n",
        "    evaluation_history.append(step_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95ktOEmRYAsb"
      },
      "outputs": [],
      "source": [
        "def generate_guided(\n",
        "    llm,\n",
        "    llm_tokenizer,\n",
        "    classifier,\n",
        "    classifier_tokenizer,\n",
        "    prompt,\n",
        "    max_len,\n",
        "    condition_lambda,\n",
        "    top_k,\n",
        "    evaluation_history=None,\n",
        "    use_z_score=False,\n",
        "    strategy=\"greedy\",\n",
        "    temperature=1.0\n",
        "):\n",
        "    device = llm.device\n",
        "\n",
        "    # ... (Steps 1, 2, and 3: Template, Sanitization, Tokenization remain same) ...\n",
        "    # [Pasted for context]\n",
        "    try:\n",
        "        if callable(CUSTOM_PROMPT_TEMPLATE):\n",
        "            messages = CUSTOM_PROMPT_TEMPLATE(prompt)\n",
        "        else:\n",
        "            messages = prompt\n",
        "    except NameError:\n",
        "        messages = prompt\n",
        "\n",
        "    if isinstance(messages, str):\n",
        "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "    elif isinstance(messages, list) and len(messages) > 0 and isinstance(messages[0], str):\n",
        "        messages = [{\"role\": \"user\", \"content\": messages[0]}]\n",
        "\n",
        "    add_gen_prompt = globals().get('ADD_GENERATION_PROMPT', True)\n",
        "    input_ids = llm_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=add_gen_prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_len):\n",
        "            # --- A: Get Base LLM Logits ---\n",
        "            llm_outputs = llm(generated_ids)\n",
        "            next_token_logits = llm_outputs.logits[:, -1, :].float()\n",
        "\n",
        "            # --- B: Get Top-K Candidates ---\n",
        "            top_logits, top_indices = torch.topk(next_token_logits, top_k)\n",
        "\n",
        "            # --- C: Run Classifier (OPTIMIZED) ---\n",
        "\n",
        "            # If lambda is effectively zero (smaller than 0.000001), skip the heavy lift\n",
        "            # Check if we are effectively turning the classifier off\n",
        "            if abs(condition_lambda) < 1e-6:\n",
        "                last_token_logits = None\n",
        "            else:\n",
        "                # Only do this heavy VRAM expansion if we actually plan to use it\n",
        "\n",
        "                # Create sequences: [Current Context + Candidate Token]\n",
        "                candidate_prefixes = torch.cat(\n",
        "                    [generated_ids.expand(top_k, -1), top_indices.squeeze(0).unsqueeze(-1)],\n",
        "                    dim=-1\n",
        "                )\n",
        "\n",
        "                # Prepare classifier batch\n",
        "                current_seq_len = candidate_prefixes.shape[1]\n",
        "                lengths = torch.LongTensor([current_seq_len] * top_k).to(device)\n",
        "                batch = [candidate_prefixes, lengths, None]\n",
        "\n",
        "                # Get raw classifier scores\n",
        "                last_token_logits = classifier.get_final_scores(batch)\n",
        "\n",
        "            # --- D: Calculate Scores (Helper 1) ---\n",
        "            # If lambda is 0, this calculates: LLM_Score + (0 * 0) = LLM_Score\n",
        "            combined_scores, clf_scores, llm_log_probs = calculate_combined_scores(\n",
        "                top_logits,\n",
        "                last_token_logits,\n",
        "                condition_lambda,\n",
        "                use_z_score\n",
        "            )\n",
        "\n",
        "            # --- E: Select Token (Helper 2) ---\n",
        "            next_token_id, best_idx = select_next_token(\n",
        "                combined_scores,\n",
        "                top_indices,\n",
        "                strategy=strategy,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            # --- F: Log (Helper 3) ---\n",
        "            if evaluation_history is not None:\n",
        "                record_evaluation(\n",
        "                    evaluation_history, step, generated_ids, llm_tokenizer,\n",
        "                    top_indices, llm_log_probs, clf_scores, combined_scores, best_idx\n",
        "                )\n",
        "\n",
        "            # --- G: Append and Yield ---\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
        "            new_text = llm_tokenizer.decode(next_token_id.squeeze(0), skip_special_tokens=True)\n",
        "\n",
        "            yield new_text\n",
        "\n",
        "            if next_token_id.item() == llm_tokenizer.eos_token_id:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGf1w6MsdO-Y"
      },
      "outputs": [],
      "source": [
        "def set_css_output_wrap():\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "    div.output_text pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "    div.output_subarea pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "    </style>\n",
        "    '''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHNgxaqRpIPe"
      },
      "outputs": [],
      "source": [
        "def prompted_testing(model_defs):\n",
        "  run = True\n",
        "  classifier, classifier_tokenizer = get_classifier(model_defs.get(\"classifier_model_name\"), model_defs.get(\"device\"))\n",
        "  while run:\n",
        "      try:\n",
        "          prompt = input(\"Enter a prompt (or 'q' to quit):\\n\")\n",
        "          if prompt.lower() == 'q':\n",
        "              break\n",
        "          condition_lambda_str = input(\"Enter a lambda value:\\n\")\n",
        "          top_k_str = input(\"Enter a top-k value:\\n\")\n",
        "          set_css_output_wrap()\n",
        "          print(f\"--- Generating with lambda={round(float(condition_lambda_str), 2)} ---\")\n",
        "\n",
        "          output_generator = generate_guided(\n",
        "              model_defs[\"llm\"],\n",
        "              model_defs[\"llm_tokenizer\"],\n",
        "              classifier,\n",
        "              classifier_tokenizer,\n",
        "              prompt,\n",
        "              MAX_NEW_TOKENS,\n",
        "              float(condition_lambda_str),\n",
        "              int(top_k_str),\n",
        "              evaluation_history=KEEP_EVALUATION_HISTORY,\n",
        "              use_z_score=True,\n",
        "              strategy=\"greedy\",  # Options: \"greedy\", \"sample\"\n",
        "              temperature=0.5     # Only used if strategy=\"sample\"\n",
        "          )\n",
        "          for new_token in output_generator:\n",
        "            print(new_token, end=\"\", flush=True)\n",
        "          print(\"\\n---\") # Add a newline at the end\n",
        "\n",
        "      except KeyboardInterrupt:\n",
        "          print(\"\\nExiting.\")\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fySBN-rhx-rU"
      },
      "outputs": [],
      "source": [
        "def targeted_testing(model_defs,\n",
        "                     prompt=\"Write the first paragraph of a mystery story.\"\n",
        "                    ):\n",
        "  classifier, classifier_tokenizer = get_classifier(model_defs.get(\"classifier_model_name\"), model_defs.get(\"device\"))\n",
        "  lambdas = [0.0, 5.0, 10.0, 20.0]\n",
        "  set_css_output_wrap()\n",
        "  for lambda_val in lambdas:\n",
        "    output_generator = generate_guided(\n",
        "              model_defs[\"llm\"],\n",
        "              model_defs[\"llm_tokenizer\"],\n",
        "              classifier,\n",
        "              classifier_tokenizer,\n",
        "              prompt,\n",
        "              MAX_NEW_TOKENS,\n",
        "              lambda_val,\n",
        "              TOP_K,\n",
        "              evaluation_history=KEEP_EVALUATION_HISTORY,\n",
        "              use_z_score=USE_Z_SCORE,\n",
        "              strategy=STRATEGY,  # Options: \"greedy\", \"sample\"\n",
        "              temperature=TEMPERATURE     # Only used if strategy=\"sample\"\n",
        "          )\n",
        "    # 2. Exhaust the generator into a single string\n",
        "    full_output = \"\".join(list(output_generator))\n",
        "    output = full_output.strip()\n",
        "    print(f\"\\nGUIDED OUTPUT (lambda={lambda_val}):\\n\", output)\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uD04G3Q4uDK"
      },
      "outputs": [],
      "source": [
        "def get_file_path(filename):\n",
        "    \"\"\"Helper to handle the Drive vs Local logic cleanly.\"\"\"\n",
        "    if SAVE_TESTS_TO_DRIVE:\n",
        "        return Path(\"/content/drive/My Drive\") / filename\n",
        "    return Path(filename)\n",
        "\n",
        "def grid_testing(model_defs):\n",
        "    prompts = [\n",
        "      \"Who was Albert Einstein?\",\n",
        "      \"Explain what artificial intelligence is.\",\n",
        "      \"How does a neural network work?\",\n",
        "      \"What are some recent advancements in renewable energy?\",\n",
        "      \"I just got promoted at work!\",\n",
        "      \"I‚Äôm feeling really down today.\",\n",
        "      \"The weather is nice today.\",\n",
        "      \"Write a short story about a robot who dreams of becoming human.\",\n",
        "      \"Generate a poem about love.\",\n",
        "      \"Give me three ideas for a birthday surprise.\",\n",
        "      \"Generate a story about space exploration.\",\n",
        "      \"Write a short story about a time traveler who meets a famous historical figure.\",\n",
        "      \"What's a good way to spend a sunny day in the park?\",\n",
        "      \"Are people fundamentally good?\",\n",
        "      \"What is the key to happiness?\",\n",
        "      \"Who is the greatest military leader in history?\",\n",
        "      \"What was life like in London at the start of the twentieth century?\",\n",
        "      \"How do you learn to play an instrument?\",\n",
        "      \"Compare and contrast impressionism and realism.\",\n",
        "      \"What causes sepsis?\",\n",
        "      \"Write a mystery story\"\n",
        "    ]\n",
        "\n",
        "    # --- 1. Setup Paths & Prompts ---\n",
        "    if TEST_PROMPTS_FILE_PATH:\n",
        "        p_file = get_file_path(TEST_PROMPTS_FILE_PATH)\n",
        "        # NOTE: In your original code you had 'filename.csv' hardcoded here\n",
        "        # instead of the variable. I fixed it to use p_file.\n",
        "        prompts_df = pd.read_csv(p_file)\n",
        "        prompts = prompts_df.iloc[:, 0].tolist()\n",
        "        if PROMPTS_TO_TEST_LIMIT:\n",
        "            prompts = prompts[:PROMPTS_TO_TEST_LIMIT]\n",
        "\n",
        "    log_file = get_file_path(f\"{GRID_TEST_RUN_NAME}_tests.csv\")\n",
        "\n",
        "    # --- 2. Initialize CSV ---\n",
        "    headers = ['model_name', 'top_k', 'lambda', 'prompt', 'elapsed_time', 'output']\n",
        "\n",
        "    # Only write headers if file doesn't exist\n",
        "    if not log_file.exists():\n",
        "        with open(log_file, 'w', newline='') as f:\n",
        "            csv.writer(f).writerow(headers)\n",
        "        print(f\"CSV file '{log_file}' created.\")\n",
        "    else:\n",
        "        print(f\"CSV file '{log_file}' appending to existing.\")\n",
        "\n",
        "    # --- 3. Define Grid ---\n",
        "    # Define your parameters here to keep the loop clean\n",
        "    lambdas = GRID_LAMBDAS\n",
        "    classifier_names = GRID_CLASSIFIER_NAMES\n",
        "    top_ks = GRID_TOP_KS\n",
        "    use_z_scores = GRID_USE_Z_SCORES\n",
        "\n",
        "    # Create a single iterable of all parameter combinations\n",
        "    # We keep classifier_name separate so we don't reload the model unnecessarily\n",
        "    param_grid = list(itertools.product(lambdas, top_ks, use_z_scores))\n",
        "\n",
        "    # --- 4. Execution Loop ---\n",
        "    # 1. Outer loop for classifiers\n",
        "    for classifier_name in tqdm(classifier_names, desc=\"Classifiers\", position=0):\n",
        "        classifier, classifier_tokenizer = get_classifier(classifier_name, model_defs.get(\"device\"))\n",
        "        tqdm.write(f\"Testing classifier: {classifier_name}\")\n",
        "        total_steps = len(prompts) * len(param_grid)\n",
        "        with tqdm(total=total_steps, desc=\"Generations\", leave=False, position=1) as pbar:\n",
        "            for prompt in prompts:\n",
        "                for lambda_val, top_k, use_z in param_grid:\n",
        "                    if abs(lambda_val) < 1e-6:\n",
        "                      print(\"Testing Baseline\")\n",
        "                      classifier_name = \"baseline\"\n",
        "                    pbar.set_description(f\"Œª:{lambda_val} | k:{top_k}\")\n",
        "                    start_time = time.time()\n",
        "                    output_generator = generate_guided(\n",
        "                                            model_defs[\"llm\"],\n",
        "                                            model_defs[\"llm_tokenizer\"],\n",
        "                                            classifier,\n",
        "                                            classifier_tokenizer,\n",
        "                                            prompt,\n",
        "                                            MAX_NEW_TOKENS,\n",
        "                                            lambda_val,\n",
        "                                            top_k,\n",
        "                                            evaluation_history=KEEP_EVALUATION_HISTORY,\n",
        "                                            use_z_score=use_z,\n",
        "                                            strategy=STRATEGY,\n",
        "                                            temperature=TEMPERATURE\n",
        "                                        )\n",
        "                    full_output = \"\".join(list(output_generator)).strip()\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    # Write results immediately (safer for long running scripts)\n",
        "                    with open(log_file, 'a', newline='') as f:\n",
        "                        csv.writer(f).writerow([\n",
        "                            classifier_name, top_k, lambda_val, prompt, elapsed_time, full_output\n",
        "                        ])\n",
        "                    pbar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOxGIgD1vHAH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def token_evaluation_testing(model_defs,\n",
        "                             prompt=\"Write the first paragraph of a mystery story.\"):\n",
        "    classifier, classifier_tokenizer = get_classifier(model_defs.get(\"classifier_model_name\"), model_defs.get(\"device\"))\n",
        "\n",
        "    # 1. Setup the capture list\n",
        "    print(\"Beginning Token Evaluation\")\n",
        "\n",
        "    # We assume 'history' is being populated inside generate_guided via a mutable list or similar mechanism\n",
        "    # If generate_guided returns history alongside tokens, adjust accordingly.\n",
        "    # Based on your snippet, it looks like 'history' might be a global or passed differently,\n",
        "    # but I will focus specifically on the timing logic here.\n",
        "\n",
        "    if KEEP_EVALUATION_HISTORY:\n",
        "      history = []\n",
        "    else:\n",
        "      history = None\n",
        "\n",
        "    output_generator = generate_guided(\n",
        "        model_defs[\"llm\"],\n",
        "        model_defs[\"llm_tokenizer\"],\n",
        "        classifier,\n",
        "        classifier_tokenizer,\n",
        "        prompt,\n",
        "        MAX_NEW_TOKENS,\n",
        "        TESTING_LAMBDA,\n",
        "        TOP_K,\n",
        "        evaluation_history=history,\n",
        "        use_z_score=USE_Z_SCORE,\n",
        "        strategy=STRATEGY,\n",
        "        temperature=TEMPERATURE\n",
        "    )\n",
        "\n",
        "    # --- NEW TIMING LOGIC START ---\n",
        "    generated_tokens_list = []\n",
        "    timing_data = []\n",
        "\n",
        "    # Start the clock before asking for the first token\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for i, token in enumerate(output_generator):\n",
        "        # The generator pauses here until the model finishes calculating the token\n",
        "\n",
        "        # Stop the clock immediately after receiving the token\n",
        "        end_time = time.perf_counter()\n",
        "\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # Log the data\n",
        "        timing_data.append({\n",
        "            \"token_index\": i,\n",
        "            \"time_seconds\": duration\n",
        "        })\n",
        "\n",
        "        generated_tokens_list.append(token)\n",
        "\n",
        "        # Reset the clock for the NEXT token\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "    # Save the timing data to a separate CSV\n",
        "    timing_df = pd.DataFrame(timing_data)\n",
        "    timing_df.to_csv(\"token_generation_times.csv\", index=False)\n",
        "    print(f\"Saved token timing logs to token_generation_times.csv\")\n",
        "\n",
        "    # Reconstruct the full string to maintain your original logic\n",
        "    full_output = \"\".join(generated_tokens_list)\n",
        "    # --- NEW TIMING LOGIC END ---\n",
        "\n",
        "    output = full_output.strip()\n",
        "    print(f\"\\nEvaluation OUTPUT:\\n\", output)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # ... The rest of your existing analysis code ...\n",
        "    step_to_analyze = EVAL_STEP_TO_ANALYZE\n",
        "    # (Ensure 'history' is accessible here. If generate_guided populated a passed list,\n",
        "    # you might need to ensure that mechanism is still working as expected.)\n",
        "    if 'history' in locals() and step_to_analyze < len(history):\n",
        "         data = history[step_to_analyze]\n",
        "         # ... existing dataframe logic ...\n",
        "         df = pd.DataFrame(data['candidates'])\n",
        "         df = df[['token_text', 'llm_score', 'classifier_score', 'weighted_combined', 'selected']]\n",
        "         print(df)\n",
        "         df.to_csv(f\"fudge_step_{step_to_analyze}_analysis.csv\", index=False)\n",
        "         print(f\"\\nSaved detailed breakdown to fudge_step_{step_to_analyze}_analysis.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wDNmpKQRI8F"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    # 1. Set the python built-in random seed\n",
        "    random.seed(seed)\n",
        "\n",
        "    # 2. Set the numpy seed\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 3. Set the pytorch seed\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # If using multi-GPU\n",
        "\n",
        "    # 4. Important: Force CuDNN to be deterministic\n",
        "    # This slows down training slightly but ensures 'exact' reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 5. Set hashing seed (vital for dictionary ordering/hashing)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    print(f\"Global seed set to {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRkZgUC4RZxU"
      },
      "outputs": [],
      "source": [
        "def setup():\n",
        "  hf_token = userdata.get('HF_TOKEN')\n",
        "  login(token=hf_token)\n",
        "\n",
        "  if SAVE_TESTS_TO_DRIVE or CLASSIFIER_PATH is not None:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI17NdCcz_0K"
      },
      "outputs": [],
      "source": [
        "def get_classifier(classifier_model_name, device):\n",
        "    model_checkpoint = f'{classifier_model_name}.pth.tar'\n",
        "\n",
        "    release_url = None\n",
        "    if GITHUB_RELEASE_VERSION is not None:\n",
        "        release_url = f\"https://github.com/latoohey/modular-fudge/releases/download/{GITHUB_RELEASE_VERSION}/{model_checkpoint}\"\n",
        "\n",
        "        # --- LOGIC ADDED HERE ---\n",
        "        if os.path.exists(model_checkpoint):\n",
        "            print(f\"Found {model_checkpoint} locally. Skipping download.\")\n",
        "        else:\n",
        "            print(f\"Downloading {model_checkpoint}...\")\n",
        "            !wget \"{release_url}\" -O {model_checkpoint}\n",
        "            print(\"Model downloaded\")\n",
        "\n",
        "        ckpt_path = model_checkpoint\n",
        "\n",
        "    else:\n",
        "        ckpt_path = os.path.join(CLASSIFIER_PATH, model_checkpoint)\n",
        "        print(\"Using Drive model\")\n",
        "\n",
        "    # 1. Load our trained LSTM classifier\n",
        "    classifier, classifier_tokenizer = load_classifier(ckpt_path, device)\n",
        "    print(\"--- Classifier loaded! ---\")\n",
        "    return classifier, classifier_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5jrIREnx7lV"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  seed_everything(SEED)\n",
        "\n",
        "  setup()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device: {device}\")\n",
        "\n",
        "  llm_model = LLM_MODEL_NAME\n",
        "  print(f\"Loading base LLM: {llm_model}...\")\n",
        "  llm = AutoModelForCausalLM.from_pretrained(\n",
        "      llm_model,\n",
        "      dtype=torch.float16\n",
        "  ).to(device)\n",
        "  llm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "      LLM_TOKENIZER_NAME\n",
        "  )\n",
        "  # Add pad token if it doesn't exist\n",
        "  if llm_tokenizer.pad_token is None:\n",
        "      # 1. Check for Llama 3 specific fine-tune token\n",
        "      if '<|finetune_right_pad_id|>' in llm_tokenizer.get_vocab():\n",
        "          llm_tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
        "\n",
        "      # 2. Check for generic reserved tokens (common in TikToken)\n",
        "      elif '<|reserved_special_token_0|>' in llm_tokenizer.get_vocab():\n",
        "          llm_tokenizer.pad_token = '<|reserved_special_token_0|>'\n",
        "\n",
        "      # 3. Safe Fallback: Use EOS token (No resizing required)\n",
        "      else:\n",
        "          print(\"Warning: No dedicated pad token found. Using EOS token as PAD.\")\n",
        "          llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "\n",
        "  print(\"--- LLM loaded! ---\")\n",
        "\n",
        "  model_defs = {\n",
        "      \"llm\": llm,\n",
        "      \"llm_tokenizer\": llm_tokenizer,\n",
        "      \"classifier_model_name\": CLASSIFIER_MODEL_NAME,\n",
        "      \"device\": device\n",
        "  }\n",
        "\n",
        "\n",
        "  if (TESTING_TYPE==\"prompted\"):\n",
        "    prompted_testing(model_defs)\n",
        "  elif (TESTING_TYPE==\"targeted\"):\n",
        "    targeted_testing(model_defs)\n",
        "  elif (TESTING_TYPE==\"grid\"):\n",
        "    grid_testing(model_defs)\n",
        "  elif (TESTING_TYPE==\"token_eval\"):\n",
        "    token_evaluation_testing(model_defs, TESTING_PROMPT)\n",
        "\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO69XPvZUMJX"
      },
      "source": [
        "## Configuration:\n",
        "\n",
        "Note: Please include your Hugging Face token as a Colab Secret named `HF_TOKEN`\n",
        "\n",
        "* There are three `TESTING_TYPE`s\n",
        "  * `grid` - tests a list of prompts - each at different lambdas\n",
        "  * `targeted` - tests a specific `TESTING_PROMPT` at different lambdas\n",
        "  * `prompted` - starts a user interface loop\n",
        "\n",
        "* `SEED` set for reproducability\n",
        "\n",
        "* Trained models are available in Releases in project GitHub repository\n",
        "https://github.com/latoohey/modular-fudge. To use one set the `CLASSIFIER_MODEL_NAME` without the file extenstions and appropriate `GITHUB_RELEASE_VERSION`. You can also use your own classifier. Just modify the download and import code below to have the `CKPT_PATH` point to the `.pth` file zipped in a `.tar`. To reduce config issues train the model with the project training script which defines the input and output needs.\n",
        "\n",
        "* Define the `CLASSIFIER_TOKENIZER_NAME` that the classifier was trained with. This does not need to match the `LLM_TOKENIZER_NAME` but re-tokenizing adds time at inference.\n",
        "\n",
        "* The `HIDDEN_DIM` is defined as an argument in training so it also needs to be supplied here.\n",
        "\n",
        "* Define the `LLM_MODEL_NAME` you want to use to generate output. The T4 GPU can comfortably run 3B parameter models and below - try the A100 for bigger models. You'll need to be approved on Hugging Face by Meta to use a Llama model.\n",
        "\n",
        "* `LLM_PAD_TOKEN` is the pad token for the LLM.\n",
        "\n",
        "* `MAX_NEW_TOKENS` sets the maximum output from the LLM. Longer outputs slow down as the process runs - I haven't figured out why yet.\n",
        "\n",
        "* `TOP_K` is the number of candidate tokens that the classifier checks before the LLM selects it's final token. The original FUDGE paper had this set at 200. Note the math on this: Each generation involves the number of output tokens (often all the way to `MAX_NEW_TOKENS`) multiplied by `TOP_K` so that number can get very big very fast.\n",
        "\n",
        "* `SAVE_TESTS_TO_DRIVE`: `True` saves files generated from the `grid` testing to your Google Drive. You'll be prompted to login. `False` saves the file to the Colab runtime. Since the test runs through about 100 generations you need to make sure your runtime doesn't expire.\n",
        "\n",
        "* To use a plain, text prompt set the `PROMPT_TEMPLATE` to `None`. Many LLMs have a defined prompt input type usually outlined in their documentation. You can define this using a lambda function named `CUSTOM_PROMPT_TEMPLATE`. It MUST accept one argument (e.g., 'p') which will be your prompt string and MUST return the 'messages' list structure you want. For example, the minimum for Llama would be defined with:\n",
        "\n",
        "  * `CUSTOM_PROMPT_TEMPLATE = lambda p: [{\"role\": \"user\", \"content\": p}]`\n",
        "\n",
        "  This should be used in conjunction with `ADD_GENERATION_PROMPT = True` which matches to the Transformers library `add_generation_prompt` argument defined [here](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE1v3a6hQ-39"
      },
      "outputs": [],
      "source": [
        "TESTING_TYPE = \"token_eval\" # \"grid\" or \"targeted\" or \"prompted\" or \"token_eval\"\n",
        "# TESTING_PROMPT is needed if TESTING_TYPE is \"targeted\" or \"token_eval\"\n",
        "TESTING_PROMPT = \"write a paragraph about north america\"\n",
        "\n",
        "GRID_TEST_RUN_NAME = \"mamba_128_4_16_1\"\n",
        "SAVE_TESTS_TO_DRIVE = True\n",
        "TEST_PROMPTS_FILE_PATH = \"modular-fudge/data/eval_prompts.csv\"\n",
        "PROMPTS_TO_TEST_LIMIT = False\n",
        "GRID_LAMBDAS = [1.4]\n",
        "GRID_CLASSIFIER_NAMES = ['mamba_128_4_16_1']\n",
        "GRID_TOP_KS = [100]\n",
        "GRID_USE_Z_SCORES = [True]\n",
        "\n",
        "#TESTING_LAMBDA is needed if TESTING_TYPE is \"token_eval\"\n",
        "TESTING_LAMBDA=0\n",
        "\n",
        "SEED = 24601\n",
        "\n",
        "CLASSIFIER_MODEL_NAME = \"lstm_2_256\"\n",
        "\n",
        "GITHUB_RELEASE_VERSION = \"v2.0\"\n",
        "#---OR---\n",
        "CLASSIFIER_PATH = '' # '/content/drive/MyDrive/modular-fudge/trained_models'\n",
        "\n",
        "KEEP_EVALUATION_HISTORY = True\n",
        "EVAL_STEP_TO_ANALYZE = 11\n",
        "\n",
        "USE_Z_SCORE = True\n",
        "STRATEGY = \"greedy\"  # Options: \"greedy\", \"sample\"\n",
        "TEMPERATURE = 1.0     # Only used if strategy=\"sample\"\n",
        "\n",
        "CLASSIFIER_TOKENIZER_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "\n",
        "# (You will need to accept the license on Hugging Face first for Llama)\n",
        "# LLM_MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "LLM_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# LLM_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "LLM_TOKENIZER_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "\n",
        "# Keep both of these values low if running \"token_eval\"\n",
        "MAX_NEW_TOKENS = 512\n",
        "TOP_K = 100\n",
        "\n",
        "# PROMPT TEMPLATE\n",
        "# To use just a plain prompt, set this to None:\n",
        "# CUSTOM_PROMPT_TEMPLATE = None\n",
        "\n",
        "# --- OR ---\n",
        "\n",
        "# To use a custom template, define a lambda function.\n",
        "# The lambda MUST accept one argument (e.g., 'p') which will be your prompt string.\n",
        "# It MUST return the 'messages' list structure you want.\n",
        "\n",
        "# Example 1: Add a simple prefix\n",
        "# CUSTOM_PROMPT_TEMPLATE = lambda p: [\n",
        "#     {\"role\": \"user\", \"content\": f\"Task: Answer the following question. {p}\"}\n",
        "# ]\n",
        "\n",
        "# Example 2: Add a System Prompt\n",
        "# This one is the base format for LLama models\n",
        "CUSTOM_PROMPT_TEMPLATE = lambda p: [{\"role\": \"user\", \"content\": p}]\n",
        "ADD_GENERATION_PROMPT = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5SoXDFX0c8V"
      },
      "outputs": [],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_frames = []\n",
        "file_pattern = re.compile(r'token_generation_times_([a-zA-Z]+)_(\\d+)\\.csv')\n",
        "\n",
        "for filename in os.listdir('.'):\n",
        "    match = file_pattern.match(filename)\n",
        "    if match:\n",
        "        raw_model_name = match.group(1) # e.g., 'lstm'\n",
        "        df = pd.read_csv(filename)\n",
        "        df['model'] = raw_model_name\n",
        "        data_frames.append(df)\n",
        "\n",
        "all_data = pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Average the tests\n",
        "averaged_data = all_data.groupby(['model', 'token_index'])['time_seconds'].mean().reset_index()\n",
        "\n",
        "print(averaged_data.head())\n",
        "\n",
        "print(averaged_data.describe())\n",
        "\n",
        "# Create a new dataframe where index is the integer, columns are categories\n",
        "df_pivoted = averaged_data.pivot(index='token_index', columns='model', values='time_seconds')\n",
        "\n",
        "print(df_pivoted.loc[10])\n",
        "\n",
        "print(df_pivoted.loc[149])\n",
        "\n",
        "print(df_pivoted.loc[299])"
      ],
      "metadata": {
        "id": "2T9qRx8eY9Xr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}