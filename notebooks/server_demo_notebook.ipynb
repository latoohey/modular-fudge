{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8SnjqAAoS0g"
      },
      "source": [
        "# Causal Classifier - Guided Generation (Testing)\n",
        "\n",
        "This notebook is used to run guided text generation using a trained causal classifier from the modular FUDGE project. It loads a base LLM and a trained classifier checkpoint to guide the output in real-time. For a complete overview of the project architecture please see the [full project on the GitHub repo](https://github.com/latoohey/modular-fudge). The training script to create a classifier for guiding generation is also available as a [Colab notebook](https://colab.research.google.com/drive/1zVfBB_zIKHpSANBmTcj8KRBFwsPgbd9m?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7wPSEbfesLd"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p292zU-3PPc2"
      },
      "outputs": [],
      "source": [
        "# For debugging as needed\n",
        "# import os\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LApwcli2o1V3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes huggingface_hub\n",
        "!pip install fastapi uvicorn pyngrok nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBf3Ztbko7Qe"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import csv\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from IPython.display import HTML, display\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from functools import lru_cache\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware # <--- Key Import\n",
        "from pydantic import BaseModel\n",
        "import time\n",
        "\n",
        "import einops\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XC_GwG5zqfjZ"
      },
      "outputs": [],
      "source": [
        "LOAD_MAMBA = True\n",
        "\n",
        "if LOAD_MAMBA:\n",
        "  # 1. Mount Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # 2. Define the directory on your Drive to store the wheels\n",
        "  #    We use a specific folder name to keep it organized.\n",
        "  wheel_dir = '/content/drive/MyDrive/colab_wheels/mamba_builds'\n",
        "  os.makedirs(wheel_dir, exist_ok=True)\n",
        "\n",
        "  # 3. Define the package versions you want\n",
        "  packages = [\n",
        "      \"causal-conv1d>=1.4.0\",\n",
        "      \"mamba-ssm\"\n",
        "  ]\n",
        "\n",
        "  # 4. Check if wheels already exist in your Drive\n",
        "  print(f\"Checking for existing wheels in {wheel_dir}...\")\n",
        "  existing_wheels = [f for f in os.listdir(wheel_dir) if f.endswith('.whl')]\n",
        "\n",
        "  if len(existing_wheels) >= len(packages):\n",
        "      print(\"‚úÖ Found pre-built wheels! Installing from Drive...\")\n",
        "      # Install directly from your Drive folder\n",
        "      !pip install \"$wheel_dir\"/*.whl\n",
        "  else:\n",
        "      print(\"‚ö†Ô∏è No wheels found. Building from source (this will take time once)...\")\n",
        "\n",
        "      # Install build dependencies first\n",
        "      !pip install packaging ninja\n",
        "\n",
        "      # Build the wheels and save them directly to your Drive\n",
        "      # We use --no-deps to avoid building wheels for huge packages like PyTorch\n",
        "      print(f\"Building wheels to {wheel_dir}...\")\n",
        "      !pip wheel {\" \".join(packages)} --wheel-dir=\"$wheel_dir\" --no-deps\n",
        "\n",
        "      # Now install the newly built wheels\n",
        "      print(\"Installing newly built wheels...\")\n",
        "      !pip install \"$wheel_dir\"/*.whl\n",
        "\n",
        "  from mamba_ssm import Mamba\n",
        "  print(\"üéâ Done! Mamba and Causal-Conv1d are ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOfiHY_3W-_Z"
      },
      "source": [
        "## Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl7FnEDiXYdu"
      },
      "outputs": [],
      "source": [
        "# --- From util.py ---\n",
        "def num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGZwvmwho-d-"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab_size, pad_token_id):\n",
        "        \"\"\"\n",
        "        Initializes the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            args: The full ArgumentParser namespace. Reads\n",
        "                  `args.lstm_hidden_dim` and `args.lstm_num_layers`.\n",
        "            vocab_size: The total vocabulary size for the embedding layer.\n",
        "            pad_token_id: The ID of the padding token.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=args.lstm_hidden_dim,\n",
        "            padding_idx=pad_token_id\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            args.lstm_hidden_dim,\n",
        "            args.lstm_hidden_dim,\n",
        "            num_layers=args.lstm_num_layers,\n",
        "            bidirectional=False,\n",
        "            dropout=0.5,\n",
        "            batch_first=True # Makes the permute/transpose logic simpler\n",
        "        )\n",
        "        self.out_linear = nn.Linear(args.lstm_hidden_dim, 1)\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        \"\"\"\n",
        "        Internal forward pass for the LSTM.\n",
        "        Requires `lengths` for sequence packing.\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_len, hidden_dim)\n",
        "        embedded_inputs = self.embed(inputs)\n",
        "\n",
        "        # Pack sequence for efficient RNN processing\n",
        "        packed_inputs = pack_padded_sequence(\n",
        "            embedded_inputs,\n",
        "            lengths.cpu(), # Must be on CPU\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # rnn_output is (packed_batch, hidden_dim)\n",
        "        rnn_output, _ = self.rnn(packed_inputs)\n",
        "\n",
        "        # Unpack: (batch_size, seq_len, hidden_dim)\n",
        "        rnn_output, _ = pad_packed_sequence(\n",
        "            rnn_output,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # (batch_size, seq_len)\n",
        "        return self.out_linear(rnn_output).squeeze(2)\n",
        "\n",
        "    def get_final_scores(self, batch):\n",
        "        \"\"\"\n",
        "        Returns the scores for the last token of each sequence in the batch.\n",
        "        Used by the guided generation to condition on the last generated token.\n",
        "        \"\"\"\n",
        "        inputs, lengths, _ = batch # _ is targets, not used here\n",
        "        # The forward method returns (batch_size, seq_len)\n",
        "        all_token_scores = self.forward(inputs, lengths)\n",
        "        # Extract the score for the last token of each sequence\n",
        "        # lengths is (batch_size,), all_token_scores is (batch_size, seq_len)\n",
        "        final_scores = all_token_scores[torch.arange(inputs.size(0)), lengths - 1]\n",
        "        return final_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhPe-suFW8GW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "========================================================================\n",
        "Mamba Classifier Model Definition\n",
        "========================================================================\n",
        "\n",
        "This file implements a Mamba-based classifier that follows the same\n",
        "contract as the LSTM classifier for compatibility with the project's\n",
        "main training (`main_train.py`) and evaluation (`evaluate.py`) scripts.\n",
        "\n",
        "The Mamba architecture uses selective state space models (SSMs) for\n",
        "efficient sequence modeling with linear complexity in sequence length.\n",
        "\n",
        "Requirements:\n",
        "- mamba-ssm (install with: pip install mamba-ssm)\n",
        "- torch\n",
        "- einops\n",
        "\"\"\"\n",
        "\n",
        "class MambaClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab_size, pad_token_id):\n",
        "        \"\"\"\n",
        "        Initializes the Mamba model.\n",
        "\n",
        "        Args:\n",
        "            args: The full ArgumentParser namespace. Reads:\n",
        "                  - `args.mamba_d_model` (hidden dimension, default 256)\n",
        "                  - `args.mamba_d_state` (SSM state dimension, default 16)\n",
        "                  - `args.mamba_d_conv` (local convolution width, default 4)\n",
        "                  - `args.mamba_expand` (expansion factor, default 2)\n",
        "                  - `args.mamba_num_layers` (number of Mamba blocks, default 4)\n",
        "                  - `args.mamba_dropout` (dropout rate, default 0.1)\n",
        "            vocab_size: The total vocabulary size for the embedding layer.\n",
        "            pad_token_id: The ID of the padding token.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Get hyperparameters from args with defaults\n",
        "        self.d_model = getattr(args, 'mamba_d_model', 256)\n",
        "        self.d_state = getattr(args, 'mamba_d_state', 16)\n",
        "        self.d_conv = getattr(args, 'mamba_d_conv', 4)\n",
        "        self.expand = getattr(args, 'mamba_expand', 2)\n",
        "        self.num_layers = getattr(args, 'mamba_num_layers', 4)\n",
        "        self.dropout_rate = getattr(args, 'mamba_dropout', 0.1)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.d_model,\n",
        "            padding_idx=pad_token_id  # Use pad_token_id from tokenizer\n",
        "        )\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "        # Stack of Mamba blocks\n",
        "        self.mamba_blocks = nn.ModuleList([\n",
        "            Mamba(\n",
        "                d_model=self.d_model,    # Model dimension\n",
        "                d_state=self.d_state,    # SSM state expansion factor\n",
        "                d_conv=self.d_conv,      # Local convolution width\n",
        "                expand=self.expand,      # Block expansion factor\n",
        "            )\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization between blocks\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(self.d_model)\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm before output\n",
        "        self.final_norm = nn.LayerNorm(self.d_model)\n",
        "\n",
        "        # Output projection to single logit per token\n",
        "        self.out_linear = nn.Linear(self.d_model, 1)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize embedding layer\n",
        "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        # Initialize linear output layer\n",
        "        nn.init.normal_(self.out_linear.weight, mean=0.0, std=0.02)\n",
        "        if self.out_linear.bias is not None:\n",
        "            nn.init.constant_(self.out_linear.bias, 0)\n",
        "\n",
        "    def forward(self, inputs, lengths=None):\n",
        "        \"\"\"\n",
        "        Internal forward pass for the Mamba model.\n",
        "\n",
        "        Note: Mamba handles variable-length sequences naturally without\n",
        "        packing/unpacking, but we accept lengths for compatibility.\n",
        "\n",
        "        Args:\n",
        "            inputs: Token IDs of shape (batch_size, seq_len)\n",
        "            lengths: Sequence lengths (optional, for compatibility)\n",
        "\n",
        "        Returns:\n",
        "            scores: Per-token logits of shape (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.shape\n",
        "\n",
        "        # Embed tokens: (batch_size, seq_len, d_model)\n",
        "        x = self.embed(inputs)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Create causal mask if needed (for padding)\n",
        "        # Mamba is inherently causal, but we need to handle padding\n",
        "        if lengths is not None:\n",
        "            # Create attention mask for padded positions\n",
        "            # Shape: (batch_size, seq_len)\n",
        "            mask = torch.arange(seq_len, device=inputs.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
        "            # Expand mask to match hidden dimension for masking\n",
        "            # Shape: (batch_size, seq_len, 1)\n",
        "            mask = mask.unsqueeze(-1).float()\n",
        "        else:\n",
        "            mask = None\n",
        "\n",
        "        # Process through Mamba blocks with residual connections\n",
        "        for i, (mamba_block, layer_norm) in enumerate(zip(self.mamba_blocks, self.layer_norms)):\n",
        "            # Pre-norm architecture\n",
        "            residual = x\n",
        "            x = layer_norm(x)\n",
        "\n",
        "            # Mamba block\n",
        "            x = mamba_block(x)\n",
        "\n",
        "            # Apply mask if available (zero out padded positions)\n",
        "            if mask is not None:\n",
        "                x = x * mask\n",
        "\n",
        "            # Residual connection and dropout\n",
        "            x = residual + self.dropout(x)\n",
        "\n",
        "        # Final normalization\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Project to logits: (batch_size, seq_len, 1) -> (batch_size, seq_len)\n",
        "        scores = self.out_linear(x).squeeze(-1)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def get_final_scores(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for evaluation.\n",
        "        Unpacks batch, calls `self.forward`, and returns final logit.\n",
        "\n",
        "        Args:\n",
        "            batch: The raw, collated batch from the DataLoader.\n",
        "\n",
        "        Returns:\n",
        "            last_logits: torch.Tensor of shape (batch_size,)\n",
        "                        The logit from the last real token for each item.\n",
        "        \"\"\"\n",
        "        # Unpack the batch\n",
        "        inputs, lengths, _ = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "\n",
        "        # Call forward pass\n",
        "        # scores shape: (batch_size, seq_len)\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Find the index of the last token for each sequence\n",
        "        # Shape: (batch_size,)\n",
        "        last_indices = (lengths - 1).long()\n",
        "\n",
        "        # Gather the scores from the last valid position\n",
        "        # Shape: (batch_size, 1) -> (batch_size,)\n",
        "        last_logits = scores.gather(\n",
        "            1, last_indices.unsqueeze(1)\n",
        "        ).squeeze(1)\n",
        "\n",
        "        return last_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB8_h0QWW8Rt"
      },
      "outputs": [],
      "source": [
        "def get_model(args, vocab_size_param, pad_token_id_param):\n",
        "    \"\"\"\n",
        "    This factory function reads the --model_type argument\n",
        "    and returns the correct, initialized model.\n",
        "    \"\"\"\n",
        "    if args.model_type == 'lstm':\n",
        "        return LSTMClassifier(args, vocab_size_param, pad_token_id_param)\n",
        "    elif args.model_type == 'mamba':\n",
        "        return MambaClassifier(args, vocab_size_param, pad_token_id_param)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {args.model_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxdjt-q1pC8u"
      },
      "outputs": [],
      "source": [
        "# 1. Add the decorator. maxsize=1 is usually enough if you just\n",
        "#    want to hold the current model in memory.\n",
        "@lru_cache(maxsize=1)\n",
        "def load_classifier(ckpt_path, device):\n",
        "    \"\"\"Loads a trained classifier from a checkpoint using the model factory.\"\"\"\n",
        "\n",
        "    # This print statement will only run the FIRST time you call the function\n",
        "    # with a specific path/device combination.\n",
        "    print(f\"Loading classifier from {ckpt_path}...\")\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Load args *from the checkpoint* to know what model to build\n",
        "    model_args = checkpoint['args']\n",
        "    print(f\"Checkpoint args: {model_args}\")\n",
        "\n",
        "    # This assumes your main_train.py saved 'tokenizer_name' in its args\n",
        "    if not hasattr(model_args, 'tokenizer_name'):\n",
        "        tokenizer_name = CLASSIFIER_TOKENIZER_NAME # Ensure this global is defined or passed in\n",
        "    else:\n",
        "        tokenizer_name = model_args.tokenizer_name\n",
        "\n",
        "    classifier_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "            # Add pad token if it doesn't exist\n",
        "    if classifier_tokenizer.pad_token is None:\n",
        "        # 1. Check for Llama 3 specific fine-tune token\n",
        "        if '<|finetune_right_pad_id|>' in classifier_tokenizer.get_vocab():\n",
        "            classifier_tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
        "\n",
        "        # 2. Check for generic reserved tokens (common in TikToken)\n",
        "        elif '<|reserved_special_token_0|>' in classifier_tokenizer.get_vocab():\n",
        "            classifier_tokenizer.pad_token = '<|reserved_special_token_0|>'\n",
        "\n",
        "        # 3. Safe Fallback: Use EOS token (No resizing required)\n",
        "        else:\n",
        "            print(\"Warning: No dedicated pad token found. Using EOS token as PAD.\")\n",
        "            classifier_tokenizer.pad_token = classifier_tokenizer.eos_token\n",
        "\n",
        "    vocab_size = len(classifier_tokenizer)\n",
        "    print(f\"Classifier vocab size: {vocab_size}\")\n",
        "    pad_token_id = classifier_tokenizer.pad_token_id\n",
        "\n",
        "    # --- Use the factory to build the correct model ---\n",
        "    # Ensure get_model is imported or defined in this scope\n",
        "    model = get_model(model_args, vocab_size, pad_token_id)\n",
        "\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Classifier loaded (Type: {model_args.model_type}, Epochs: {checkpoint['epoch']}).\")\n",
        "\n",
        "    # Returns the tuple. The cache will store this entire tuple.\n",
        "    return model, classifier_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1md5jU1WkNr1"
      },
      "outputs": [],
      "source": [
        "def calculate_combined_scores(top_logits, last_token_logits, condition_lambda, use_z_score=False):\n",
        "    \"\"\"\n",
        "    Normalizes and combines LLM logits with Classifier scores.\n",
        "    Returns: combined_log_probs (for selection), final_classifier_scores (for logging), llm_log_probs\n",
        "    \"\"\"\n",
        "    # 1. Normalize LLM scores to log probs\n",
        "    llm_log_probs = F.log_softmax(top_logits, dim=-1)\n",
        "\n",
        "    # --- CHANGE 1: EARLY EXIT FOR OPTIMIZATION ---\n",
        "    # If the classifier was skipped (lambda=0), return pure LLM scores immediately.\n",
        "    if last_token_logits is None:\n",
        "        # Create dummy zeros for the \"classifier scores\" so the logger doesn't crash.\n",
        "        # We make it match the shape of top_logits [1, top_k]\n",
        "        dummy_classifier_scores = torch.zeros_like(top_logits)\n",
        "\n",
        "        # Return: (Pure LLM Scores, Dummy Zeros, Pure LLM Scores)\n",
        "        return llm_log_probs, dummy_classifier_scores, llm_log_probs\n",
        "\n",
        "    # 2. Normalize Classifier scores to log probs\n",
        "    classifier_log_probs = F.log_softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # Extract the \"True\" class score (assuming binary classification index 1 is target)\n",
        "    if len(classifier_log_probs.shape) > 1 and classifier_log_probs.shape[-1] > 1:\n",
        "        relevant_classifier_scores = classifier_log_probs[:, 1]\n",
        "    else:\n",
        "        relevant_classifier_scores = classifier_log_probs\n",
        "\n",
        "    # 3. Apply Strategy\n",
        "    if use_z_score:\n",
        "        # Calculate stats across the top_k candidates\n",
        "        c_mean = relevant_classifier_scores.mean()\n",
        "\n",
        "        # --- CHANGE 2: FIX THE STD() CRASH ---\n",
        "        # unbiased=False prevents crash when top_k=1 (div by zero error)\n",
        "        c_std = relevant_classifier_scores.std(unbiased=False)\n",
        "\n",
        "        if c_std < 1e-8: c_std = 1.0 # Safety\n",
        "\n",
        "        final_classifier_scores = (relevant_classifier_scores - c_mean) / c_std\n",
        "    else:\n",
        "        final_classifier_scores = relevant_classifier_scores\n",
        "\n",
        "    # 4. Combine: LLM_Log_Prob + (Lambda * Classifier_Score)\n",
        "    combined_log_probs = llm_log_probs + (condition_lambda * final_classifier_scores)\n",
        "\n",
        "    return combined_log_probs, final_classifier_scores, llm_log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcPwJLgCTx7l"
      },
      "outputs": [],
      "source": [
        "def select_next_token(combined_log_probs, top_indices, strategy=\"greedy\", temperature=1.0):\n",
        "    \"\"\"\n",
        "    Selects the next token index based on strategy.\n",
        "    Returns: next_token_id (tensor), best_index_relative (int index of top_k)\n",
        "    \"\"\"\n",
        "    if strategy == \"sample\":\n",
        "        # Divide by temp to control randomness\n",
        "        probs = F.softmax(combined_log_probs / temperature, dim=-1)\n",
        "        # Sample from the distribution\n",
        "        best_index_relative = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "        # Greedy default (Returns a 0-dim scalar tensor)\n",
        "        best_index_relative = torch.argmax(combined_log_probs)\n",
        "\n",
        "    # --- THE FIX IS HERE ---\n",
        "    # We force convert to Python int regardless of dimensions.\n",
        "    # argmax returns 0-dim, multinomial returns 2-dim. .item() handles both.\n",
        "    if isinstance(best_index_relative, torch.Tensor):\n",
        "        best_index_relative = int(best_index_relative.item())\n",
        "\n",
        "    # Extract the actual token ID from the top_k list\n",
        "    next_token_id = top_indices[0, best_index_relative].unsqueeze(0)\n",
        "\n",
        "    return next_token_id, best_index_relative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJNtN0PGT2_T"
      },
      "outputs": [],
      "source": [
        "def record_evaluation(evaluation_history, step, generated_ids, tokenizer,\n",
        "                      top_indices, llm_scores, clf_scores, combined_scores, selected_idx):\n",
        "    \"\"\"\n",
        "    Logs the step details to the history list.\n",
        "    \"\"\"\n",
        "    current_context_str = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    top_k = top_indices.shape[1]\n",
        "\n",
        "    step_data = {\n",
        "        \"step\": step,\n",
        "        \"context\": current_context_str,\n",
        "        \"candidates\": []\n",
        "    }\n",
        "\n",
        "    # FIX: Normalize clf_scores to be 1D so we can loop over it easily\n",
        "    # If it came from zeros_like(top_logits), it's [1, K]. We want [K].\n",
        "    if clf_scores.dim() > 1:\n",
        "        clf_scores = clf_scores.squeeze(0)\n",
        "\n",
        "    for i in range(top_k):\n",
        "        cand_id = top_indices[0, i].item()\n",
        "        cand_token = tokenizer.decode([cand_id])\n",
        "\n",
        "        # Safe extraction of scalar values\n",
        "        s_llm = llm_scores[0, i].item()\n",
        "\n",
        "        # FIX: Now we can safely use [i] for both Normal and Optimized cases\n",
        "        s_clf = clf_scores[i].item()\n",
        "\n",
        "        s_comb = combined_scores[0, i].item()\n",
        "        is_winner = (i == selected_idx)\n",
        "\n",
        "        step_data[\"candidates\"].append({\n",
        "            \"token_text\": cand_token,\n",
        "            \"llm_score\": round(s_llm, 4),\n",
        "            \"classifier_score\": round(s_clf, 4),\n",
        "            \"weighted_combined\": round(s_comb, 4),\n",
        "            \"selected\": is_winner\n",
        "        })\n",
        "\n",
        "    evaluation_history.append(step_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95ktOEmRYAsb"
      },
      "outputs": [],
      "source": [
        "def generate_guided(\n",
        "    llm,\n",
        "    llm_tokenizer,\n",
        "    classifier,\n",
        "    classifier_tokenizer,\n",
        "    prompt,\n",
        "    max_len,\n",
        "    condition_lambda,\n",
        "    top_k,\n",
        "    evaluation_history=None,\n",
        "    use_z_score=False,\n",
        "    strategy=\"greedy\",\n",
        "    temperature=1.0\n",
        "):\n",
        "    device = llm.device\n",
        "\n",
        "    # ... (Steps 1, 2, and 3: Template, Sanitization, Tokenization remain same) ...\n",
        "    # [Pasted for context]\n",
        "    try:\n",
        "        if callable(CUSTOM_PROMPT_TEMPLATE):\n",
        "            messages = CUSTOM_PROMPT_TEMPLATE(prompt)\n",
        "        else:\n",
        "            messages = prompt\n",
        "    except NameError:\n",
        "        messages = prompt\n",
        "\n",
        "    if isinstance(messages, str):\n",
        "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "    elif isinstance(messages, list) and len(messages) > 0 and isinstance(messages[0], str):\n",
        "        messages = [{\"role\": \"user\", \"content\": messages[0]}]\n",
        "\n",
        "    add_gen_prompt = globals().get('ADD_GENERATION_PROMPT', True)\n",
        "    input_ids = llm_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=add_gen_prompt,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_len):\n",
        "            # --- A: Get Base LLM Logits ---\n",
        "            llm_outputs = llm(generated_ids)\n",
        "            next_token_logits = llm_outputs.logits[:, -1, :].float()\n",
        "\n",
        "            # --- B: Get Top-K Candidates ---\n",
        "            top_logits, top_indices = torch.topk(next_token_logits, top_k)\n",
        "\n",
        "            # --- C: Run Classifier (OPTIMIZED) ---\n",
        "\n",
        "            # If lambda is effectively zero (smaller than 0.000001), skip the heavy lift\n",
        "            # Check if we are effectively turning the classifier off\n",
        "            if abs(condition_lambda) < 1e-6:\n",
        "                last_token_logits = None\n",
        "            else:\n",
        "                # Only do this heavy VRAM expansion if we actually plan to use it\n",
        "\n",
        "                # Create sequences: [Current Context + Candidate Token]\n",
        "                candidate_prefixes = torch.cat(\n",
        "                    [generated_ids.expand(top_k, -1), top_indices.squeeze(0).unsqueeze(-1)],\n",
        "                    dim=-1\n",
        "                )\n",
        "\n",
        "                # Prepare classifier batch\n",
        "                current_seq_len = candidate_prefixes.shape[1]\n",
        "                lengths = torch.LongTensor([current_seq_len] * top_k).to(device)\n",
        "                batch = [candidate_prefixes, lengths, None]\n",
        "\n",
        "                # Get raw classifier scores\n",
        "                last_token_logits = classifier.get_final_scores(batch)\n",
        "\n",
        "            # --- D: Calculate Scores (Helper 1) ---\n",
        "            # If lambda is 0, this calculates: LLM_Score + (0 * 0) = LLM_Score\n",
        "            combined_scores, clf_scores, llm_log_probs = calculate_combined_scores(\n",
        "                top_logits,\n",
        "                last_token_logits,\n",
        "                condition_lambda,\n",
        "                use_z_score\n",
        "            )\n",
        "\n",
        "            # --- E: Select Token (Helper 2) ---\n",
        "            next_token_id, best_idx = select_next_token(\n",
        "                combined_scores,\n",
        "                top_indices,\n",
        "                strategy=strategy,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            # --- F: Log (Helper 3) ---\n",
        "            if evaluation_history is not None:\n",
        "                record_evaluation(\n",
        "                    evaluation_history, step, generated_ids, llm_tokenizer,\n",
        "                    top_indices, llm_log_probs, clf_scores, combined_scores, best_idx\n",
        "                )\n",
        "\n",
        "            # --- G: Append and Yield ---\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
        "            new_text = llm_tokenizer.decode(next_token_id.squeeze(0), skip_special_tokens=True)\n",
        "\n",
        "            yield new_text\n",
        "\n",
        "            if next_token_id.item() == llm_tokenizer.eos_token_id:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wDNmpKQRI8F"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    # 1. Set the python built-in random seed\n",
        "    random.seed(seed)\n",
        "\n",
        "    # 2. Set the numpy seed\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 3. Set the pytorch seed\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # If using multi-GPU\n",
        "\n",
        "    # 4. Important: Force CuDNN to be deterministic\n",
        "    # This slows down training slightly but ensures 'exact' reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 5. Set hashing seed (vital for dictionary ordering/hashing)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    print(f\"Global seed set to {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRkZgUC4RZxU"
      },
      "outputs": [],
      "source": [
        "def setup():\n",
        "  from google.colab import drive\n",
        "  from google.colab import userdata\n",
        "  seed_everything(SEED)\n",
        "  drive.mount('/content/drive')\n",
        "  hf_token = userdata.get('HF_TOKEN')\n",
        "  login(token=hf_token)\n",
        "  NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "  return NGROK_AUTH_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI17NdCcz_0K"
      },
      "outputs": [],
      "source": [
        "def get_classifier(classifier_model_name, device):\n",
        "\n",
        "  model_checkpoint = f'{classifier_model_name}.pth.tar'\n",
        "\n",
        "  release_url = None\n",
        "  if GITHUB_RELEASE_VERSION is not None:\n",
        "    release_url = f\"https://github.com/latoohey/modular-fudge/releases/download/{GITHUB_RELEASE_VERSION}/{model_checkpoint}\"\n",
        "    # To use your own model update the path here\n",
        "\n",
        "  if GITHUB_RELEASE_VERSION is not None:\n",
        "    !wget \"{release_url}\" -O {model_checkpoint}\n",
        "    ckpt_path = model_checkpoint\n",
        "    print(\"Model downloaded\")\n",
        "  else:\n",
        "    ckpt_path = os.path.join(CLASSIFIER_PATH, model_checkpoint)\n",
        "    print(\"Using Drive model\")\n",
        "\n",
        "\n",
        "  # 1. Load our trained LSTM classifier\n",
        "  classifier, classifier_tokenizer = load_classifier(ckpt_path, device)\n",
        "  print(\"--- Classifier loaded! ---\")\n",
        "  return classifier, classifier_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5jrIREnx7lV"
      },
      "outputs": [],
      "source": [
        "def initialize_environment():\n",
        "\n",
        "  NGROK_AUTH_TOKEN = setup()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device: {device}\")\n",
        "\n",
        "  llm_model = LLM_MODEL_NAME\n",
        "  print(f\"Loading base LLM: {llm_model}...\")\n",
        "  llm = AutoModelForCausalLM.from_pretrained(\n",
        "      llm_model,\n",
        "      dtype=torch.float16\n",
        "  ).to(device)\n",
        "  llm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "      LLM_TOKENIZER_NAME\n",
        "  )\n",
        "  # Add pad token if it doesn't exist\n",
        "  if llm_tokenizer.pad_token is None:\n",
        "      # 1. Check for Llama 3 specific fine-tune token\n",
        "      if '<|finetune_right_pad_id|>' in llm_tokenizer.get_vocab():\n",
        "          llm_tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
        "\n",
        "      # 2. Check for generic reserved tokens (common in TikToken)\n",
        "      elif '<|reserved_special_token_0|>' in llm_tokenizer.get_vocab():\n",
        "          llm_tokenizer.pad_token = '<|reserved_special_token_0|>'\n",
        "\n",
        "      # 3. Safe Fallback: Use EOS token (No resizing required)\n",
        "      else:\n",
        "          print(\"Warning: No dedicated pad token found. Using EOS token as PAD.\")\n",
        "          llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "\n",
        "  print(\"--- LLM loaded! ---\")\n",
        "\n",
        "  server_classifiers = {}\n",
        "\n",
        "  for classifier_name in SERVER_CLASSIFIERS:\n",
        "    classifier, classifier_tokenizer = get_classifier(classifier_name, device)\n",
        "    server_classifiers[classifier_name] = {\n",
        "        'classifier': classifier,\n",
        "        'classifier_tokenizer': classifier_tokenizer\n",
        "    }\n",
        "\n",
        "\n",
        "  model_defs = {\n",
        "      \"llm\": llm,\n",
        "      \"llm_tokenizer\": llm_tokenizer,\n",
        "      \"classifier\": None,\n",
        "      \"classifer_tokenizer\": None\n",
        "  }\n",
        "\n",
        "  prompt_args = {\n",
        "    \"prompt\": None,\n",
        "    \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "    \"lambda_val\": None,\n",
        "    \"top_k\": TOP_K,\n",
        "    \"evaluation_history\": KEEP_EVALUATION_HISTORY,\n",
        "    \"use_z_score\": USE_Z_SCORE,\n",
        "    \"strategy\": STRATEGY,\n",
        "    \"temperature\": TEMPERATURE\n",
        "  }\n",
        "\n",
        "  return Mamba, model_defs, server_classifiers, prompt_args, NGROK_AUTH_TOKEN\n",
        "\n",
        "def initialize_environment_test():\n",
        "    NGROK_AUTH_TOKEN = setup()\n",
        "    Mamba = None\n",
        "    model_defs = {\n",
        "      \"llm\": \"llm\",\n",
        "      \"llm_tokenizer\": \"llm_tokenizer\",\n",
        "      \"classifier\": None,\n",
        "      \"classifer_tokenizer\": None\n",
        "    }\n",
        "\n",
        "    prompt_args = {\n",
        "      \"prompt\": None,\n",
        "      \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "      \"lambda_val\": None,\n",
        "      \"top_k\": TOP_K,\n",
        "      \"evaluation_history\": KEEP_EVALUATION_HISTORY,\n",
        "      \"use_z_score\": USE_Z_SCORE,\n",
        "      \"strategy\": STRATEGY,\n",
        "      \"temperature\": TEMPERATURE\n",
        "    }\n",
        "\n",
        "    server_classifiers = {\n",
        "        \"classifier_name\": {\n",
        "            \"classifier\": \"classifier\",\n",
        "            \"classifier_tokenizer\": \"classifier_tokenizer\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return Mamba, model_defs, server_classifiers, prompt_args, NGROK_AUTH_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO69XPvZUMJX"
      },
      "source": [
        "## Configuration:\n",
        "\n",
        "Note: Please include your Hugging Face token as a Colab Secret named `HF_TOKEN`\n",
        "\n",
        "* There are three `TESTING_TYPE`s\n",
        "  * `grid` - tests a list of prompts - each at different lambdas\n",
        "  * `targeted` - tests a specific `TESTING_PROMPT` at different lambdas\n",
        "  * `prompted` - starts a user interface loop\n",
        "\n",
        "* `SEED` set for reproducability\n",
        "\n",
        "* Trained models are available in Releases in project GitHub repository\n",
        "https://github.com/latoohey/modular-fudge. To use one set the `CLASSIFIER_MODEL_NAME` without the file extenstions and appropriate `GITHUB_RELEASE_VERSION`. You can also use your own classifier. Just modify the download and import code below to have the `CKPT_PATH` point to the `.pth` file zipped in a `.tar`. To reduce config issues train the model with the project training script which defines the input and output needs.\n",
        "\n",
        "* Define the `CLASSIFIER_TOKENIZER_NAME` that the classifier was trained with. This does not need to match the `LLM_TOKENIZER_NAME` but re-tokenizing adds time at inference.\n",
        "\n",
        "* The `HIDDEN_DIM` is defined as an argument in training so it also needs to be supplied here.\n",
        "\n",
        "* Define the `LLM_MODEL_NAME` you want to use to generate output. The T4 GPU can comfortably run 3B parameter models and below - try the A100 for bigger models. You'll need to be approved on Hugging Face by Meta to use a Llama model.\n",
        "\n",
        "* `LLM_PAD_TOKEN` is the pad token for the LLM.\n",
        "\n",
        "* `MAX_NEW_TOKENS` sets the maximum output from the LLM. Longer outputs slow down as the process runs - I haven't figured out why yet.\n",
        "\n",
        "* `TOP_K` is the number of candidate tokens that the classifier checks before the LLM selects it's final token. The original FUDGE paper had this set at 200. Note the math on this: Each generation involves the number of output tokens (often all the way to `MAX_NEW_TOKENS`) multiplied by `TOP_K` so that number can get very big very fast.\n",
        "\n",
        "* `SAVE_TESTS_TO_DRIVE`: `True` saves files generated from the `grid` testing to your Google Drive. You'll be prompted to login. `False` saves the file to the Colab runtime. Since the test runs through about 100 generations you need to make sure your runtime doesn't expire.\n",
        "\n",
        "* To use a plain, text prompt set the `PROMPT_TEMPLATE` to `None`. Many LLMs have a defined prompt input type usually outlined in their documentation. You can define this using a lambda function named `CUSTOM_PROMPT_TEMPLATE`. It MUST accept one argument (e.g., 'p') which will be your prompt string and MUST return the 'messages' list structure you want. For example, the minimum for Llama would be defined with:\n",
        "\n",
        "  * `CUSTOM_PROMPT_TEMPLATE = lambda p: [{\"role\": \"user\", \"content\": p}]`\n",
        "\n",
        "  This should be used in conjunction with `ADD_GENERATION_PROMPT = True` which matches to the Transformers library `add_generation_prompt` argument defined [here](https://huggingface.co/docs/transformers/en/chat_templating#addgenerationprompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE1v3a6hQ-39"
      },
      "outputs": [],
      "source": [
        "TESTING_TYPE = \"grid\" # \"grid\" or \"targeted\" or \"prompted\" or \"token_eval\"\n",
        "# TESTING_PROMPT is needed if TESTING_TYPE is \"targeted\" or \"token_eval\"\n",
        "TESTING_PROMPT = \"write a paragraph about europe\"\n",
        "\n",
        "GRID_TEST_RUN_NAME = \"mamba__128_4_16_1\"\n",
        "SAVE_TESTS_TO_DRIVE = True\n",
        "TEST_PROMPTS_FILE_PATH = \"modular-fudge/data/eval_prompts.csv\"\n",
        "PROMPTS_TO_TEST_LIMIT = 50\n",
        "GRID_LAMBDAS = [1.2, 1.4, 1.6]\n",
        "GRID_CLASSIFIER_NAMES = ['mamba_128_4_16_1']\n",
        "GRID_TOP_KS = [100]\n",
        "GRID_USE_Z_SCORES = [True]\n",
        "\n",
        "#TESTING_LAMBDA is needed if TESTING_TYPE is \"token_eval\"\n",
        "TESTING_LAMBDA=1\n",
        "\n",
        "SEED = 24601\n",
        "\n",
        "CLASSIFIER_MODEL_NAME = \"mamba_256_4_16_1\"\n",
        "\n",
        "GITHUB_RELEASE_VERSION = None # \"v1.0\"\n",
        "#---OR---\n",
        "CLASSIFIER_PATH = '/content/drive/MyDrive/modular-fudge/trained_models'\n",
        "\n",
        "KEEP_EVALUATION_HISTORY = None\n",
        "EVAL_STEP_TO_ANALYZE = None\n",
        "\n",
        "USE_Z_SCORE = True\n",
        "STRATEGY = \"greedy\"  # Options: \"greedy\", \"sample\"\n",
        "TEMPERATURE = 1.0     # Only used if strategy=\"sample\"\n",
        "\n",
        "CLASSIFIER_TOKENIZER_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "\n",
        "# (You will need to accept the license on Hugging Face first for Llama)\n",
        "# LLM_MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "LLM_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# LLM_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "LLM_TOKENIZER_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "\n",
        "# Keep both of these values low if running \"token_eval\"\n",
        "MAX_NEW_TOKENS = 512\n",
        "TOP_K = 100\n",
        "\n",
        "# PROMPT TEMPLATE\n",
        "# To use just a plain prompt, set this to None:\n",
        "# CUSTOM_PROMPT_TEMPLATE = None\n",
        "\n",
        "# --- OR ---\n",
        "\n",
        "# To use a custom template, define a lambda function.\n",
        "# The lambda MUST accept one argument (e.g., 'p') which will be your prompt string.\n",
        "# It MUST return the 'messages' list structure you want.\n",
        "\n",
        "# Example 1: Add a simple prefix\n",
        "# CUSTOM_PROMPT_TEMPLATE = lambda p: [\n",
        "#     {\"role\": \"user\", \"content\": f\"Task: Answer the following question. {p}\"}\n",
        "# ]\n",
        "\n",
        "# Example 2: Add a System Prompt\n",
        "# This one is the base format for LLama models\n",
        "CUSTOM_PROMPT_TEMPLATE = lambda p: [{\"role\": \"user\", \"content\": p}]\n",
        "ADD_GENERATION_PROMPT = True\n",
        "\n",
        "SERVER_CLASSIFIERS = [\"mamba_128_4_16_1\", \"lstm_2_256\"]\n",
        "#SERVER_CLASSIFIERS = [\"lstm_2_256\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map this to the arguments your generate_guided function needs\n",
        "class GenerationArgs(BaseModel):\n",
        "    prompt: str\n",
        "    lambda_val: float = 1.0\n",
        "    model_name: str = None\n",
        "\n",
        "# --- 3. Your Existing Generator Logic ---\n",
        "# I'm mocking this for the example, but you will PASTE YOUR FUNCTION here.\n",
        "def generate_guided_test(args):\n",
        "    import time\n",
        "    mock_tokens = [\"This \", \"is \", \"a \", \"streamed \", \"response \", \"from \", \"Colab.\"]\n",
        "    for token in mock_tokens:\n",
        "        time.sleep(0.5)\n",
        "        yield token\n",
        "\n",
        "# --- 4. The Streaming Wrapper ---\n",
        "# This is the bridge. It takes the HTTP input, calls your function,\n",
        "# and ensures the output is formatted correctly for the web stream.\n",
        "def stream_generator(input_data: GenerationArgs):\n",
        "    # We pass the pydantic object or unpack it to your function\n",
        "    # generator = generate_guided(input_data.dict()) # If passing dict\n",
        "\n",
        "    # Calling your generator\n",
        "    client_data = input_data.model_dump()\n",
        "    # generator = generate_guided_test(client_data)\n",
        "\n",
        "    prompt_args['prompt'] = client_data['prompt']\n",
        "    prompt_args['lambda_val'] = client_data['lambda_val']\n",
        "\n",
        "    try:\n",
        "      requested_classifier = server_classifiers[client_data['model_name']]\n",
        "    except KeyError:\n",
        "      requested_classifier = next(iter(server_classifiers.values()))\n",
        "\n",
        "    model_defs['classifier'] = requested_classifier['classifier']\n",
        "    model_defs['classifier_tokenizer'] = requested_classifier['classifier_tokenizer']\n",
        "\n",
        "    generator = generate_guided(\n",
        "        model_defs[\"llm\"],\n",
        "        model_defs[\"llm_tokenizer\"],\n",
        "        model_defs[\"classifier\"],\n",
        "        model_defs[\"classifier_tokenizer\"],\n",
        "        prompt_args['prompt'],\n",
        "        prompt_args['max_new_tokens'],\n",
        "        prompt_args['lambda_val'],\n",
        "        prompt_args['top_k'],\n",
        "        prompt_args['evaluation_history'],\n",
        "        prompt_args['use_z_score'],\n",
        "        prompt_args['strategy'],\n",
        "        prompt_args['temperature']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    for text_chunk in generator:\n",
        "        # We yield the text directly.\n",
        "        # Browsers/Clients will receive this chunk by chunk.\n",
        "        yield text_chunk\n",
        "\n",
        "\n",
        "Mamba, model_defs, server_classifiers, prompt_args, NGROK_AUTH_TOKEN = initialize_environment()\n",
        "\n",
        "# Mamba, model_defs, server_classifiers, prompt_args, NGROK_AUTH_TOKEN = initialize_environment_test()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# 4. CRITICAL: Add CORS Middleware\n",
        "# This tells FastAPI to answer the \"OPTIONS\" preflight check with \"Yes, come on in.\"\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],      # Allows all origins (VS Code, GitHub Pages, etc.)\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],      # Allows all methods (POST, GET, OPTIONS, etc.)\n",
        "    allow_headers=[\"*\"],      # Allows all headers (including ngrok-skip-browser-warning)\n",
        ")\n",
        "\n",
        "# --- 5. The Endpoint ---\n",
        "@app.post(\"/generate_stream\")\n",
        "async def generate_stream_endpoint(input_data: GenerationArgs):\n",
        "    # We return a StreamingResponse object\n",
        "    # media_type=\"text/event-stream\" is standard for streaming updates,\n",
        "    # but \"text/plain\" is often easier for simple raw text demos.\n",
        "    return StreamingResponse(stream_generator(input_data), media_type=\"text/plain\")\n",
        "\n",
        "\n",
        "# --- 6. Tunnel & Run ---\n",
        "ngrok.kill()\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Connect on port 8000\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"\\nüöÄ API LIVE AT: {public_url} üöÄ\")\n",
        "print(f\"Endpoint: {public_url}/generate_stream\")\n",
        "\n",
        "# 8. Run the Server (The Async Way)\n",
        "# This prevents the 'asyncio.run() cannot be called' error\n",
        "config = uvicorn.Config(app, port=8000)\n",
        "server = uvicorn.Server(config)\n",
        "await server.serve()"
      ],
      "metadata": {
        "id": "oyKLCJ3gdwS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "XQu7KB9OrA8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}