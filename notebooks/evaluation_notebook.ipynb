{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install nltk\n",
        "# !pip install --upgrade setuptools"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AUyU91bFnoQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import textstat\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "5Y36VQxrpg18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. NLTK Setup ---\n",
        "def ensure_nltk_resources():\n",
        "    resources = ['punkt_tab', 'punkt']\n",
        "    for res in resources:\n",
        "        try:\n",
        "            nltk.data.find(f'tokenizers/{res}')\n",
        "        except LookupError:\n",
        "            print(f\"Downloading missing NLTK resource: {res}...\")\n",
        "            nltk.download(res, quiet=True)\n",
        "\n",
        "ensure_nltk_resources()"
      ],
      "metadata": {
        "id": "utCFTd5xpjFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Configuration & Constants ---\n",
        "# defined in one place to ensure consistency across all printing/saving functions\n",
        "METRIC_CONFIG = {\n",
        "    # Metric Name             # Higher is Better? (True/False)\n",
        "    'perplexity':             False,\n",
        "    'coherence':              True,\n",
        "    'dist_1':                 True,\n",
        "    'dist_2':                 True,\n",
        "    'dist_3':                 True,\n",
        "    'style_score':            True,\n",
        "    'token_length':           True,  # Assuming preference for length, change to False if brevity preferred\n",
        "    'avg_sent_length':        True,  # For formal style transfer, usually higher\n",
        "    'avg_word_length':        True,\n",
        "    'type_token_ratio':       True,\n",
        "    'long_word_ratio':        True,\n",
        "    'flesch_reading_ease':    False, # Lower = more complex/formal\n",
        "    'flesch_kincaid_grade':   True,  # Higher = more complex\n",
        "    'complex_sentence_ratio': True,\n",
        "    'passive_voice_ratio':    False, # Context dependent, but usually minimize in active writing\n",
        "    'elapsed_time':           False\n",
        "}\n",
        "\n",
        "STYLE_MOVEMENT_CHECKS = [\n",
        "    ('avg_sent_length', 'increase'),\n",
        "    ('avg_word_length', 'increase'),\n",
        "    ('long_word_ratio', 'increase'),\n",
        "    ('flesch_reading_ease', 'decrease'),\n",
        "    ('complex_sentence_ratio', 'increase'),\n",
        "]\n"
      ],
      "metadata": {
        "id": "kMrX6uUApoym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. The Evaluator Class ---\n",
        "class StyleTransferEvaluator:\n",
        "    def __init__(self, style_model_path, device=None):\n",
        "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Initializing Evaluator on {self.device}...\")\n",
        "\n",
        "        self.style_model_path = style_model_path\n",
        "        self._load_models()\n",
        "\n",
        "    def _load_models(self):\n",
        "        # 1. Fluency (GPT-2 XL)\n",
        "        print(\"Loading GPT-2 XL (Fluency)...\")\n",
        "        self.ppl_tokenizer = AutoTokenizer.from_pretrained('gpt2-xl')\n",
        "        self.ppl_model = AutoModelForCausalLM.from_pretrained('gpt2-xl').to(self.device)\n",
        "        self.ppl_model.eval()\n",
        "\n",
        "        # 2. Coherence (SBERT)\n",
        "        print(\"Loading SBERT (Coherence)...\")\n",
        "        self.coherence_model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
        "\n",
        "        # 3. Style Classifier (Custom)\n",
        "        print(f\"Loading Style Classifier from {self.style_model_path}...\")\n",
        "        self.style_tokenizer = AutoTokenizer.from_pretrained(self.style_model_path, local_files_only=True)\n",
        "        self.style_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.style_model_path, local_files_only=True\n",
        "        ).to(self.device)\n",
        "        self.style_model.eval()\n",
        "\n",
        "    # --- Calculation Methods ---\n",
        "\n",
        "    def _calc_perplexity(self, text):\n",
        "        if not text or not text.strip(): return 0.0\n",
        "        encodings = self.ppl_tokenizer(text, return_tensors='pt')\n",
        "        input_ids = encodings.input_ids.to(self.device)\n",
        "        if input_ids.size(1) > 1024: input_ids = input_ids[:, :1024]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.ppl_model(input_ids, labels=input_ids)\n",
        "            return torch.exp(outputs.loss).item()\n",
        "\n",
        "    def _calc_style_score(self, text):\n",
        "        if not text: return 0.0\n",
        "        inputs = self.style_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.style_model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            return probs[0][1].item()\n",
        "\n",
        "    def _calc_coherence(self, text):\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        if len(sentences) < 2: return 0.0\n",
        "        embeddings = self.coherence_model.encode(sentences, convert_to_tensor=True)\n",
        "        cosine_scores = util.cos_sim(embeddings, embeddings)\n",
        "        # Average adjacent scores\n",
        "        adjacent_scores = [cosine_scores[i][i+1].item() for i in range(len(sentences)-1)]\n",
        "        return np.mean(adjacent_scores).item()\n",
        "\n",
        "    def _calc_distinct_n(self, text, n):\n",
        "        tokens = text.lower().split()\n",
        "        if len(tokens) < n: return 0.0\n",
        "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "        if not ngrams: return 0.0\n",
        "        return len(set(ngrams)) / len(ngrams)\n",
        "\n",
        "    def _calc_stylometrics(self, text):\n",
        "        # Default safe return\n",
        "        defaults = {k: 0.0 for k in METRIC_CONFIG.keys() if k not in ['perplexity', 'coherence', 'style_score', 'dist_1', 'dist_2', 'dist_3', 'token_length', 'elapsed_time']}\n",
        "        if not text or not text.strip(): return defaults\n",
        "\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        words = []\n",
        "        for sent in sentences:\n",
        "            words.extend(nltk.word_tokenize(sent.lower()))\n",
        "        words_alpha = [w for w in words if w.isalpha()] or ['placeholder']\n",
        "        sent_lengths = [len(nltk.word_tokenize(s)) for s in sentences]\n",
        "\n",
        "        # Passive Voice Logic (Preserved exactly)\n",
        "        passive_indicators = {'was', 'were', 'been', 'being', 'be'}\n",
        "        passive_count = 0\n",
        "        for sent in sentences:\n",
        "            sent_words = nltk.word_tokenize(sent.lower())\n",
        "            for i in range(len(sent_words) - 1):\n",
        "                if sent_words[i] in passive_indicators:\n",
        "                    if sent_words[i+1].endswith('ed'):\n",
        "                        passive_count += 1\n",
        "                        break\n",
        "\n",
        "        return {\n",
        "            'avg_sent_length': np.mean(sent_lengths).item(),\n",
        "            'avg_word_length': np.mean([len(w) for w in words_alpha]).item(),\n",
        "            'type_token_ratio': len(set(words_alpha)) / len(words_alpha),\n",
        "            'long_word_ratio': sum(1 for w in words_alpha if len(w) > 6) / len(words_alpha),\n",
        "            'flesch_reading_ease': textstat.flesch_reading_ease(text) if textstat.flesch_reading_ease(text) is not None else 0.0,\n",
        "            'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text) if textstat.flesch_kincaid_grade(text) is not None else 0.0,\n",
        "            'complex_sentence_ratio': sum(1 for l in sent_lengths if l > 20) / len(sentences),\n",
        "            'passive_voice_ratio': passive_count / len(sentences)\n",
        "        }\n",
        "\n",
        "    # --- Core Processing Methods ---\n",
        "\n",
        "    def evaluate_text(self, input_data):\n",
        "        \"\"\"\n",
        "        Accepts either a string or a dict (row). Returns a dict of metrics.\n",
        "        \"\"\"\n",
        "        # Handle input types\n",
        "        if isinstance(input_data, dict):\n",
        "            text = input_data.get('output', \"\")\n",
        "            result = input_data.copy()\n",
        "        else:\n",
        "            text = input_data\n",
        "            result = {'output': text}\n",
        "\n",
        "        # Calculate FUDGE metrics\n",
        "        result['perplexity'] = self._calc_perplexity(text)\n",
        "        result['coherence'] = self._calc_coherence(text)\n",
        "        result['dist_1'] = self._calc_distinct_n(text, 1)\n",
        "        result['dist_2'] = self._calc_distinct_n(text, 2)\n",
        "        result['dist_3'] = self._calc_distinct_n(text, 3)\n",
        "        result['style_score'] = self._calc_style_score(text)\n",
        "        result['token_length'] = len(self.style_tokenizer.encode(text))\n",
        "\n",
        "        # Calculate Stylometric metrics\n",
        "        stylometrics = self._calc_stylometrics(text)\n",
        "        result.update(stylometrics)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def evaluate_file(self, input_path, output_path, treatments=['model_name', 'top_k', 'lambda']):\n",
        "        \"\"\"\n",
        "        Reads CSV, processes rows, prints reports, saves CSV.\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(input_path)\n",
        "        print(f\"Processing {len(df)} rows from {input_path}...\")\n",
        "\n",
        "\n",
        "#        with open('splits/test/eb.txt', 'r', encoding='utf-8') as f:\n",
        "#            lines = [line.strip() for line in f]\n",
        "#        # Construct the dictionary first\n",
        "#        data = {\n",
        "#            'model_name': 'eb',\n",
        "#            'top_k': 100,\n",
        "#            'lambda': 0,\n",
        "#            'prompt': 'Prompt',\n",
        "#            'elapsed_time': 1,\n",
        "#            'output': lines\n",
        "#        }\n",
        "#        # Create the DataFrame\n",
        "#        df = pd.DataFrame(data)\n",
        "\n",
        "        results = []\n",
        "        for record in tqdm(df.to_dict('records')):\n",
        "            results.append(self.evaluate_text(record))\n",
        "\n",
        "        result_df = pd.DataFrame(results)\n",
        "\n",
        "        # Generate Reports\n",
        "        print(\"\\n=== Evaluation Summary ===\")\n",
        "        print_metrics_summary(result_df, treatments)\n",
        "\n",
        "        if 'model_name' in result_df.columns and 'baseline' in result_df['model_name'].values:\n",
        "            print(\"\\n=== Style Movement (vs Baseline) ===\")\n",
        "            print_style_movement(result_df)\n",
        "\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved results to {output_path}\")\n",
        "        return result_df\n",
        "\n"
      ],
      "metadata": {
        "id": "vzB2sJRFpcmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Reporting Helpers ---\n",
        "\n",
        "def print_metrics_summary(df, treatments):\n",
        "    # Identify which metrics exist in this dataframe\n",
        "    available_metrics = [m for m in METRIC_CONFIG.keys() if m in df.columns]\n",
        "\n",
        "    # Group by treatments\n",
        "    try:\n",
        "        summary = df.groupby(treatments)[available_metrics].mean().round(2)\n",
        "    except KeyError:\n",
        "        # Fallback if treatments columns missing\n",
        "        summary = df[available_metrics].mean().to_frame().T.round(2)\n",
        "\n",
        "    print(summary)\n",
        "    print(\"-\" * 60)\n",
        "    print(\"üèÜ BEST PERFORMERS\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for metric in available_metrics:\n",
        "        higher_is_better = METRIC_CONFIG.get(metric, True)\n",
        "\n",
        "        # Find best index\n",
        "        if higher_is_better:\n",
        "            best_idx = summary[metric].idxmax()\n",
        "            direction = \"High\"\n",
        "        else:\n",
        "            best_idx = summary[metric].idxmin()\n",
        "            direction = \"Low\"\n",
        "\n",
        "        best_val = summary.loc[best_idx, metric]\n",
        "\n",
        "        # Format label\n",
        "        label = \", \".join(map(str, best_idx)) if isinstance(best_idx, tuple) else str(best_idx)\n",
        "        print(f\"‚Ä¢ {metric:<22} ({direction}): {label} ({best_val})\")\n"
      ],
      "metadata": {
        "id": "JB_qEfQUpyn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_style_movement(df):\n",
        "    baseline_df = df[df['model_name'] == 'baseline']\n",
        "    if baseline_df.empty: return\n",
        "\n",
        "    for model in df['model_name'].unique():\n",
        "        if model == 'baseline': continue\n",
        "\n",
        "        model_df = df[df['model_name'] == model]\n",
        "        print(f\"\\n{model} vs Baseline:\")\n",
        "\n",
        "        for metric, expected_dir in STYLE_MOVEMENT_CHECKS:\n",
        "            if metric not in df.columns: continue\n",
        "\n",
        "            base_mean = baseline_df[metric].mean()\n",
        "            model_mean = model_df[metric].mean()\n",
        "            change = model_mean - base_mean\n",
        "            pct = (change / base_mean * 100) if base_mean != 0 else 0\n",
        "\n",
        "            is_good = (expected_dir == 'increase' and change > 0) or \\\n",
        "                      (expected_dir == 'decrease' and change < 0)\n",
        "            symbol = \"‚úì\" if is_good else \"‚úó\"\n",
        "\n",
        "            print(f\"  {metric}: {base_mean:.2f} ‚Üí {model_mean:.2f} ({pct:+.1f}%) {symbol}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ssC0GzWHp4Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. File Management Helpers ---\n",
        "\n",
        "def merge_evaluation_reports(project_path):\n",
        "    \"\"\"Finds all eval_* CSVs, merges them, and generates a master summary.\"\"\"\n",
        "    folder = Path(project_path)\n",
        "    csv_files = list(folder.glob('eval_*.csv'))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(\"No eval files found to merge.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Merging {len(csv_files)} files...\")\n",
        "    df_list = [pd.read_csv(f) for f in csv_files]\n",
        "    combined = pd.concat(df_list, ignore_index=True).drop_duplicates()\n",
        "\n",
        "    print(f\"Final dataset shape: {combined.shape}\")\n",
        "\n",
        "    # Run the report on the combined data\n",
        "    print_metrics_summary(combined, ['model_name', 'top_k', 'lambda'])\n",
        "\n",
        "    output_file = folder / 'combined_evals.csv'\n",
        "    combined.to_csv(output_file, index=False)\n",
        "    print(f\"Combined report saved to {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tiV2fDysp8YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_colab_helper(project_path, judge_path=None):\n",
        "    \"\"\"Helper to handle the specific pathing needs of Colab.\"\"\"\n",
        "    if 'google.colab' in sys.modules:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        base = '/content/drive/My Drive/'\n",
        "\n",
        "        # Return updated paths\n",
        "        p_path = f\"{base}{project_path}\"\n",
        "        j_path = f\"{base}{judge_path}\" if judge_path else None\n",
        "        return p_path, j_path\n",
        "    return project_path, judge_path\n",
        "\n"
      ],
      "metadata": {
        "id": "0LqGLcSrp_26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH_IN = \"modular-fudge/data/test_results\"\n",
        "JUDGE_PATH_IN = \"modular-fudge/judge_classifier\"\n",
        "DATA_FILE = \"llm_baseline_tests.csv\"\n",
        "SINGLE_TEXT =  \"\"\"Geology is the scientific study of the Earth's physical structure, composition, and processes that shape its surface. It encompasses various disciplines, including physics, chemistry, biology, and environmental science, to understand the complex interactions between the Earth's lithosphere (rock), hydrosphere (water), atmosphere (air), and biosphere (living organisms).\"\"\"\n",
        "SINGLE_TEXT = \"\"\"Strawberries are a popular and tasty fruit. They are typically red and have a sweet flavor that many people enjoy. Unlike some other fruits, strawberries have tiny seeds on their outer surface. Because of their good taste, people eat them in many ways, such as fresh, in a fruit salad, or made into jams or jellies.\"\"\""
      ],
      "metadata": {
        "id": "dyg5p-xbqDdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_project_path, full_judge_path = mount_colab_helper(PROJECT_PATH_IN, JUDGE_PATH_IN)"
      ],
      "metadata": {
        "id": "EEJJ5GvFsqlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = StyleTransferEvaluator(full_judge_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BPfNlwPPsFgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTION A: Batch Process a CSV\n",
        "# evaluator.evaluate_file(\n",
        "#  input_path=f\"{full_project_path}/{DATA_FILE}\",\n",
        "#  output_path=f\"{full_project_path}/eval_{DATA_FILE}\")\n",
        "# print('Evaluation Complete')\n",
        "# OPTION B: Single Text\n",
        "# result = evaluator.evaluate_text(SINGLE_TEXT)\n",
        "# print(\"=== Evaluation Summary ===\")\n",
        "# for key, value in result.items():\n",
        "#   if key != 'output':\n",
        "#     print(f\"{key}: {value:.3f}\")\n",
        "\n",
        "# OPTION C: Merge Files\n",
        "merge_evaluation_reports(full_project_path)"
      ],
      "metadata": {
        "id": "xe17x7Tjqt78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}