{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Classifier - Model Training\n",
        "\n",
        "This notebook is used to train a causal classifier (e.g., LSTM) as part of the modular FUDGE project. It handles data loading, model initialization, and the complete training loop. You can run all cells to train a model from scratch. For a complete overview of the project architecture and how this script is used, please see the [full project on the GitHub repo](https://github.com/latoohey/modular-fudge). The testing script to create a classifier for guiding generation is also available as a [Colab notebook](https://colab.research.google.com/drive/1RlbP7iAJY4Ajga93tRy0KjxPwe558Yb9?usp=sharing)."
      ],
      "metadata": {
        "id": "gCZNkLAmtUCa"
      },
      "id": "gCZNkLAmtUCa"
    },
    {
      "cell_type": "markdown",
      "id": "9f4c0861-e283-4c40-bad9-16de50eae553",
      "metadata": {
        "id": "9f4c0861-e283-4c40-bad9-16de50eae553"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64cbf53-d971-4563-8268-b44db401c5f6",
      "metadata": {
        "collapsed": true,
        "id": "c64cbf53-d971-4563-8268-b44db401c5f6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers scikit-learn torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Mamba related packages are slow to build\n",
        "# Leave these installs commented unless you intend to train a Mamba model\n",
        "#\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the directory on your Drive to store the wheels\n",
        "#    We use a specific folder name to keep it organized.\n",
        "wheel_dir = '/content/drive/MyDrive/colab_wheels/mamba_builds'\n",
        "os.makedirs(wheel_dir, exist_ok=True)\n",
        "\n",
        "# 3. Define the package versions you want\n",
        "packages = [\n",
        "    \"causal-conv1d>=1.4.0\",\n",
        "    \"mamba-ssm\"\n",
        "]\n",
        "\n",
        "# 4. Check if wheels already exist in your Drive\n",
        "print(f\"Checking for existing wheels in {wheel_dir}...\")\n",
        "existing_wheels = [f for f in os.listdir(wheel_dir) if f.endswith('.whl')]\n",
        "\n",
        "if len(existing_wheels) >= len(packages):\n",
        "    print(\"âœ… Found pre-built wheels! Installing from Drive...\")\n",
        "    # Install directly from your Drive folder\n",
        "    !pip install \"$wheel_dir\"/*.whl\n",
        "else:\n",
        "    print(\"âš ï¸ No wheels found. Building from source (this will take time once)...\")\n",
        "\n",
        "    # Install build dependencies first\n",
        "    !pip install packaging ninja\n",
        "\n",
        "    # Build the wheels and save them directly to your Drive\n",
        "    # We use --no-deps to avoid building wheels for huge packages like PyTorch\n",
        "    print(f\"Building wheels to {wheel_dir}...\")\n",
        "    !pip wheel {\" \".join(packages)} --wheel-dir=\"$wheel_dir\" --no-deps\n",
        "\n",
        "    # Now install the newly built wheels\n",
        "    print(\"Installing newly built wheels...\")\n",
        "    !pip install \"$wheel_dir\"/*.whl\n",
        "\n",
        "print(\"ðŸŽ‰ Done! Mamba and Causal-Conv1d are ready.\")\n",
        "from mamba_ssm import Mamba\n",
        "import einops"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F82JG2M-JE3V"
      },
      "id": "F82JG2M-JE3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8249b979-e6f4-452c-91eb-be9897a27473",
      "metadata": {
        "id": "8249b979-e6f4-452c-91eb-be9897a27473"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import IterableDataset, DataLoader, Dataset as TorchDataset\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict\n",
        "from types import SimpleNamespace\n",
        "import sys\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ef150e-da7f-494b-aaf9-671fb7abb026",
      "metadata": {
        "id": "46ef150e-da7f-494b-aaf9-671fb7abb026"
      },
      "source": [
        "## Configuration:\n",
        "\n",
        "Note: Please include your Hugging Face token as a Colab Secret named `HF_TOKEN`\n",
        "\n",
        "* Set a `SEED` set for reproducability\n",
        "\n",
        "* `DATA_DIR` is the location where your data is saved it must contain train and test folders and have training data split into text files by class. E.g:\n",
        "  * data/splits/train/style_0.txt\n",
        "  * data/splits/train/style_1.txt\n",
        "  * data/splits/test/style_0.txt\n",
        "  * data/splits/test/style_1.txt\n",
        "\n",
        "  The project data can be downloaded from Github by setting the `PROJECT_DATA` parameter to True. This will handle creating the necessary directories.\n",
        "\n",
        "* `SAVE_DIR` is the folder where your saved model package will be saved. You'll be prompted to log into Google Drive as part of the flow.\n",
        "\n",
        "* `HF_TOKEN` this can be left blank when running on Colab\n",
        "* `CKPT` Leave this set to None in Colab\n",
        "\n",
        "* There are a variety of general training hyperparameters to set including `BATCH_SIZE`, `EPOCHS`, `LR`, `NUM_WORKERS`, and output `PRINT_FREQ`\n",
        "\n",
        "* The `POS_CAT` and `NEG_CAT` should match the names of the respective .txt files in your train and text folders. They define which category will be the training target.\n",
        "\n",
        "* `MODEL_TYPE` is the code for the type of model you'll be training. This must match one of the codes in the `get_model()` factory function.\n",
        "\n",
        "* Models themselves can be any type of binary classifier but they must have:\n",
        "  1.  A class that inherits from `torch.nn.Module`.\n",
        "  2.  An `__init__` method with a specific signature.\n",
        "  3.  An *internal* `forward` method for the model's logic.\n",
        "  4.  A `get_scores_for_batch` \"adapter\" method for training.\n",
        "  5.  A `get_final_scores` \"adapter\" method for evaluation.\n",
        "\n",
        "  The full description of model requirements is included [below at the top of the Models section](https://colab.research.google.com/drive/1zVfBB_zIKHpSANBmTcj8KRBFwsPgbd9m#scrollTo=f0717723-a806-42b1-81b4-50de25dd7e15).\n",
        "\n",
        "* Settings like `LSTM_HIDDEN_DIM` and `LSTM_NUM_LAYERS` are specific to particular model types. You can define more parameters as needed provided that you add them to `traing_args` namespace below which will give access in your model definition.\n",
        "\n",
        "* Define the `TOKENIZER_NAME` and `PAD_TOKEN` your data will be tokenized into before training.\n",
        "\n",
        "* `VAL_SIZE` defines the size of the validation set used during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9420de19-d5eb-4f9a-a4a1-e9935f1315a4",
      "metadata": {
        "id": "9420de19-d5eb-4f9a-a4a1-e9935f1315a4"
      },
      "outputs": [],
      "source": [
        "SEED=24601\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "MODEL_TYPE=\"mamba\"\n",
        "\n",
        "DATA_DIR=\"data\"\n",
        "SAVE_DIR=\"trained_models\"\n",
        "\n",
        "HF_TOKEN=\"XXXXX\" # Set automatically from secret if on colab\n",
        "\n",
        "BATCH_SIZE=64\n",
        "EPOCHS=100\n",
        "LR=0.0001\n",
        "NUM_WORKERS=4\n",
        "CKPT=None\n",
        "PRINT_FREQ=100\n",
        "\n",
        "# --- Tokenizer ---\n",
        "TOKENIZER_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "\n",
        "# --- Data Processing ---\n",
        "POS_CAT=\"eb\"\n",
        "NEG_CAT=\"simple_wiki\"\n",
        "VAL_SIZE = 400 # total across both datasets\n",
        "MAX_LEN = 1024\n",
        "MIN_SENTENCE_LENGTH = 3\n",
        "\n",
        "# --- LSTM Specific ---\n",
        "LSTM_HIDDEN_DIM=128\n",
        "LSTM_NUM_LAYERS=4\n",
        "\n",
        "# --- Mamba Specific ---\n",
        "MAMBA_D_MODEL=128\n",
        "MAMBA_NUM_LAYERS=4\n",
        "MAMBA_D_STATE=16\n",
        "MAMBA_DROPOUT=0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60200097-ede5-45cc-ac3d-fd84023437cb",
      "metadata": {
        "id": "60200097-ede5-45cc-ac3d-fd84023437cb"
      },
      "source": [
        "## Calculated Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34658b2-481f-4b03-a118-af6d4b2e3ec9",
      "metadata": {
        "id": "c34658b2-481f-4b03-a118-af6d4b2e3ec9"
      },
      "outputs": [],
      "source": [
        "def setup():\n",
        "\n",
        "  training_args = SimpleNamespace(\n",
        "      data_dir=DATA_DIR,\n",
        "      save_dir=SAVE_DIR,\n",
        "      hf_token=HF_TOKEN,\n",
        "      ckpt=CKPT,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      lr=LR,\n",
        "      seed=SEED,\n",
        "      num_workers=NUM_WORKERS,\n",
        "      pos_cat=POS_CAT,\n",
        "      neg_cat=NEG_CAT,\n",
        "      print_freq=PRINT_FREQ,\n",
        "      model_type=MODEL_TYPE,\n",
        "      lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
        "      lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "      mamba_d_model=MAMBA_D_MODEL,\n",
        "      mamba_num_layers=MAMBA_NUM_LAYERS,\n",
        "      mamba_d_state=MAMBA_D_STATE,\n",
        "      mamba_dropout=MAMBA_DROPOUT,\n",
        "      on_colab=False,\n",
        "      val_size=VAL_SIZE,\n",
        "      max_len=MAX_LEN,\n",
        "      min_sentence_length=MIN_SENTENCE_LENGTH,\n",
        "      tokenizer_name=TOKENIZER_NAME\n",
        "  )\n",
        "\n",
        "  if 'google.colab' in sys.modules:\n",
        "    training_args.on_colab = True\n",
        "    from google.colab import drive, userdata\n",
        "    drive.mount('/content/drive')\n",
        "    training_args.hf_token = userdata.get('HF_TOKEN')\n",
        "    training_args.save_dir = \"/content/drive/My Drive/\"\n",
        "\n",
        "  return training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5354363-ebfa-4f8c-a646-f40eb12d8eed",
      "metadata": {
        "id": "f5354363-ebfa-4f8c-a646-f40eb12d8eed"
      },
      "source": [
        "## util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d79654-70dd-47b7-a22c-b5da7ba70bb7",
      "metadata": {
        "id": "45d79654-70dd-47b7-a22c-b5da7ba70bb7"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, save_path):\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    torch.save(state, save_path)\n",
        "    print(f\"Checkpoint saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3997efd3-0aff-4b62-9d7e-fba83e422da8",
      "metadata": {
        "id": "3997efd3-0aff-4b62-9d7e-fba83e422da8"
      },
      "outputs": [],
      "source": [
        "def num_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee5191c-c09f-43af-b9b8-e15d3bfa6e0c",
      "metadata": {
        "id": "aee5191c-c09f-43af-b9b8-e15d3bfa6e0c"
      },
      "outputs": [],
      "source": [
        "def pad_mask(lengths: torch.LongTensor) -> torch.BoolTensor:\n",
        "    \"\"\"\n",
        "    Create a mask of batch x seq where 1 is for non-padding\n",
        "    and 0 is for padding.\n",
        "    \"\"\"\n",
        "    device = lengths.device\n",
        "    max_len = lengths.max().item() # .item() converts Tensor to int\n",
        "\n",
        "    # 1. Create range [0, 1, 2, ... max_len-1]\n",
        "    # Shape: (1, max_len)\n",
        "    ids = torch.arange(max_len, device=device).unsqueeze(0)\n",
        "\n",
        "    # 2. Compare with lengths\n",
        "    # Shape: (batch_size, 1)\n",
        "    # Broadcasting: (1, max_len) < (batch_size, 1) -> (batch_size, max_len)\n",
        "    mask = ids < lengths.unsqueeze(1)\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b15a76e-fba2-4530-885d-643e5b65d3b5",
      "metadata": {
        "id": "9b15a76e-fba2-4530-885d-643e5b65d3b5"
      },
      "outputs": [],
      "source": [
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries.append(time.ctime(time.time()))\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cada0ad-dc60-4486-aa80-8007757c4310",
      "metadata": {
        "id": "2cada0ad-dc60-4486-aa80-8007757c4310"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        if self.count > 0:\n",
        "            self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b657bf93-b952-46c6-baa6-37ed39df4c14",
      "metadata": {
        "id": "b657bf93-b952-46c6-baa6-37ed39df4c14"
      },
      "source": [
        "## data.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_github(split, category):\n",
        "    \"\"\"\n",
        "    Downloads a specific data file from the GitHub repo,\n",
        "    skipping if the file already exists.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Define the URL and the local output path\n",
        "    base_url = \"https://raw.githubusercontent.com/latoohey/modular-fudge/refs/heads/main/data/splits/\"\n",
        "    filename = f\"{category}.txt\"\n",
        "    url = f\"{base_url}{split}/{category}/{filename}\"\n",
        "\n",
        "    out_dir = Path(\"splits\") / split\n",
        "    out_file_path = out_dir / filename\n",
        "\n",
        "    # --- ADDED CHECK ---\n",
        "    # 2. Check if the file already exists before doing anything else\n",
        "    if out_file_path.is_file():\n",
        "        print(f\"File {out_file_path} already exists. Skipping download.\")\n",
        "        return  # Exit the function early\n",
        "    # -------------------\n",
        "\n",
        "    try:\n",
        "        # 3. Create the directory (if it doesn't exist)\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # 4. Make the HTTP request\n",
        "        print(f\"Downloading {url}...\")\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # 5. Check if the request was successful\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # 6. Write the content to the file\n",
        "        out_file_path.write_bytes(response.content)\n",
        "\n",
        "        print(f\"Successfully saved to {out_file_path}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing file: {e}\")\n"
      ],
      "metadata": {
        "id": "5MiUxSRFxsV1"
      },
      "id": "5MiUxSRFxsV1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660383f3-de1a-44e9-b985-d4196ae933c3",
      "metadata": {
        "id": "660383f3-de1a-44e9-b985-d4196ae933c3"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    \"\"\"\n",
        "    Updated collate function that pads with the correct pad_id\n",
        "    instead of hardcoding 0.\n",
        "    \"\"\"\n",
        "    # Retrieve the dynamic pad_id passed from the SplitLoader (Index 2)\n",
        "    pad_id = batch[0][2]\n",
        "\n",
        "    inputs = [b[0] for b in batch]\n",
        "    lengths = torch.LongTensor([b[1] for b in batch])\n",
        "    max_length = lengths.max()\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "        diff = max_length - len(inputs[i])\n",
        "\n",
        "        if diff > 0:\n",
        "            # --- FIX ---\n",
        "            # Use torch.full to create a tensor filled with the specific pad_id\n",
        "            padding = torch.full((diff,), pad_id, dtype=torch.long)\n",
        "            inputs[i] = torch.cat([inputs[i], padding], dim=0)\n",
        "\n",
        "    inputs = torch.stack(inputs, dim=0)\n",
        "\n",
        "    # Get the single integer label (index 3)\n",
        "    classification_labels = [b[3] for b in batch]\n",
        "    classification_labels = torch.LongTensor(classification_labels)\n",
        "\n",
        "    return (inputs, lengths, classification_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ba61a7-001c-4aa8-8738-ba08bf1a7ba3",
      "metadata": {
        "id": "b1ba61a7-001c-4aa8-8738-ba08bf1a7ba3"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, args):\n",
        "        print('Loading data...')\n",
        "        self.data_dir = args.data_dir\n",
        "        self.max_len = getattr(args, 'max_len', 512)\n",
        "\n",
        "        # 1. Tokenizer Setup\n",
        "        tokenizer_name = getattr(args, 'tokenizer_name', 'distilbert-base-uncased')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "        # 2. Pad Token Logic (Restored exactly)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            if '<|finetune_right_pad_id|>' in self.tokenizer.get_vocab():\n",
        "                self.tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
        "            elif '<|reserved_special_token_0|>' in self.tokenizer.get_vocab():\n",
        "                self.tokenizer.pad_token = '<|reserved_special_token_0|>'\n",
        "            else:\n",
        "                print(\"Warning: No dedicated pad token found. Using EOS token as PAD.\")\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.tokenizer_pad_id = self.tokenizer.pad_token_id\n",
        "        print(f\"Dataset initialized. Pad Token: {self.tokenizer.pad_token} (ID: {self.tokenizer_pad_id})\")\n",
        "\n",
        "        # 3. Data Loading\n",
        "        train, val, test = [], [], []\n",
        "\n",
        "        # Helper to determine split size\n",
        "        # Tries to find args.val_size, falls back to global VAL_SIZE, or 0\n",
        "        target_val_size = getattr(args, 'val_size', globals().get('VAL_SIZE', 0))\n",
        "\n",
        "        # --- LOAD TRAIN & VAL ---\n",
        "        for category, label in [(args.pos_cat, 1), (args.neg_cat, 0)]:\n",
        "\n",
        "            # A. Path Logic (Restored)\n",
        "            if getattr(args, 'on_colab', False):\n",
        "                file_path = os.path.join('splits', 'train', f'{category}.txt')\n",
        "            else:\n",
        "                file_path = os.path.join(self.data_dir, 'splits', 'train', f'{category}.txt')\n",
        "\n",
        "            # B. Download Logic (Restored)\n",
        "            if not os.path.exists(file_path):\n",
        "                if getattr(args, 'on_colab', False):\n",
        "                    print(f\"Downloading train/{category} from GitHub...\")\n",
        "                    # Assumes get_data_from_github is defined in your main script context\n",
        "                    get_data_from_github('train', category)\n",
        "                else:\n",
        "                    print(f\"Warning: Train file not found at {file_path}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "            # C. Processing\n",
        "            if os.path.exists(file_path):\n",
        "                with open(file_path, 'r', encoding='utf-8') as rf:\n",
        "                    # We load all lines first to handle the split correctly\n",
        "                    # We also apply the MIN_LENGTH filter here to keep Map-Style efficient\n",
        "                    lines = [line.strip() for line in rf if len(line.strip().split()) >= args.min_sentence_length]\n",
        "\n",
        "                    # Split Logic: First (val_size // 2) items go to Val\n",
        "                    # If val_size is 0/None, we default to 10% split\n",
        "                    limit = target_val_size // 2 if target_val_size > 0 else len(lines) // 10\n",
        "\n",
        "                    val.extend([(l, label) for l in lines[:limit]])\n",
        "                    train.extend([(l, label) for l in lines[limit:]])\n",
        "\n",
        "        # --- LOAD TEST ---\n",
        "        for category, label in [(args.pos_cat, 1), (args.neg_cat, 0)]:\n",
        "\n",
        "            # A. Path Logic\n",
        "            if getattr(args, 'on_colab', False):\n",
        "                file_path = os.path.join('splits', 'test', f'{category}.txt')\n",
        "            else:\n",
        "                file_path = os.path.join(self.data_dir, 'splits', 'test', f'{category}.txt')\n",
        "\n",
        "            # B. Download Logic\n",
        "            if not os.path.exists(file_path):\n",
        "                if getattr(args, 'on_colab', False):\n",
        "                    print(f\"Downloading test/{category} from GitHub...\")\n",
        "                    get_data_from_github('test', category)\n",
        "                else:\n",
        "                    print(f\"Warning: Test file not found at {file_path}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "            # C. Processing\n",
        "            if os.path.exists(file_path):\n",
        "                with open(file_path, 'r', encoding='utf-8') as rf:\n",
        "                    lines = [line.strip() for line in rf if len(line.strip().split()) >= args.min_sentence_length]\n",
        "                    test.extend([(l, label) for l in lines])\n",
        "\n",
        "        self.splits = {'train': train, 'val': val, 'test': test}\n",
        "        print(f\"Counts - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
        "\n",
        "    def shuffle(self, split, seed=None):\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "        random.shuffle(self.splits[split])\n",
        "\n",
        "    def loader(self, split, batch_size, num_workers=0, mode='fudge'):\n",
        "        data_source = self.splits[split]\n",
        "\n",
        "        if mode == 'judge':\n",
        "            collate_fn = self.collate_for_judge\n",
        "        else:\n",
        "            collate_fn = self.collate_for_fudge\n",
        "\n",
        "        return DataLoader(\n",
        "            SplitLoader(data_source, self),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "\n",
        "    # --- COLLATORS ---\n",
        "    def collate_for_fudge(self, batch):\n",
        "        input_ids_list = [item[0] for item in batch]\n",
        "        labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
        "\n",
        "        padded_inputs = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids_list, batch_first=True, padding_value=self.tokenizer_pad_id\n",
        "        )\n",
        "        lengths = torch.tensor([len(x) for x in input_ids_list], dtype=torch.long)\n",
        "        return padded_inputs, lengths, labels\n",
        "\n",
        "    def collate_for_judge(self, batch):\n",
        "        input_ids_list = [item[0] for item in batch]\n",
        "        labels = [item[1] for item in batch]\n",
        "\n",
        "        padded_inputs = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids_list, batch_first=True, padding_value=self.tokenizer_pad_id\n",
        "        )\n",
        "        attention_mask = (padded_inputs != self.tokenizer_pad_id).long()\n",
        "        return {\n",
        "            'input_ids': padded_inputs,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ead21f6-3b92-4804-acee-8ef44615429f",
      "metadata": {
        "id": "6ead21f6-3b92-4804-acee-8ef44615429f"
      },
      "outputs": [],
      "source": [
        "class SplitLoader(TorchDataset):\n",
        "    def __init__(self, data, parent):\n",
        "        self.data = data\n",
        "        self.parent = parent\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raw_sentence, label = self.data[idx]\n",
        "\n",
        "        encoded = self.parent.tokenizer(\n",
        "            raw_sentence,\n",
        "            truncation=True,\n",
        "            max_length=self.parent.max_len,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        return torch.tensor(encoded['input_ids'], dtype=torch.long), label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0717723-a806-42b1-81b4-50de25dd7e15",
      "metadata": {
        "id": "f0717723-a806-42b1-81b4-50de25dd7e15"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f07521-eb3f-4c31-a819-82d9af6b1dfb",
      "metadata": {
        "id": "c2f07521-eb3f-4c31-a819-82d9af6b1dfb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "========================================================================\n",
        "Model Definition File Contract\n",
        "========================================================================\n",
        "\n",
        "This file defines a classifier model architecture that is compatible with\n",
        "the project's main training (`main_train.py`) and evaluation (`evaluate.py`)\n",
        "scripts.\n",
        "\n",
        "To add a new model (e.g., \"MyNewModel\"), create a new file like this one\n",
        "(e.g., `models\\my_new_model.py`) and implement the following components:\n",
        "\n",
        "1.  A class that inherits from `torch.nn.Module`.\n",
        "2.  An `__init__` method with a specific signature.\n",
        "3.  An *internal* `forward` method for the model's logic.\n",
        "4.  A `get_scores_for_batch` \"adapter\" method for training.\n",
        "5.  A `get_final_scores` \"adapter\" method for evaluation.\n",
        "\n",
        "The factory in `models\\__init__.py` must also be updated to import\n",
        "and select this new class based on the `--model_type` argument.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "CONTRACT DETAILS\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "--- [1. `__init__` Method] ---\n",
        "\n",
        "The `__init__` method *must* have the following signature:\n",
        "\n",
        "def __init__(self, args, vocab_size, pad_id):\n",
        "    ...\n",
        "\n",
        "    - `args`: The fully populated `ArgumentParser` namespace. This\n",
        "      object will contain all command-line arguments, allowing the\n",
        "      model to pull its own specific hyperparameters (e.g.,\n",
        "      `args.my_model_hidden_dim`, `args.my_model_num_layers`).\n",
        "\n",
        "    - `vocab_size`: An integer (e.g., from `len(tokenizer)`)\n",
        "      specifying the total vocabulary size.\n",
        "\n",
        "    - `pad_id`: The integer index of the padding token.\n",
        "      The model MUST use this to set `padding_idx` in the\n",
        "      `nn.Embedding` layer. This ensures the embedding for\n",
        "      padding is fixed to zero, which is critical for\n",
        "      proper loss calculation and gradient safety.\n",
        "\n",
        "--- [2. `forward` Method] ---\n",
        "\n",
        "The `forward` method is *internal* to your model. Its signature can\n",
        "be whatever you need.\n",
        "\n",
        "    - Example: `def forward(self, inputs, lengths):` (for LSTM)\n",
        "    - Example: `def forward(self, inputs):` (for Mamba/Transformer)\n",
        "\n",
        "This method will contain the core architectural logic (embeddings,\n",
        "RNN/Mamba/Transformer layers, output head).\n",
        "\n",
        "It *must* be causal (unidirectional) and output a tensor of\n",
        "per-token logits.\n",
        "\n",
        "    - **Output Shape:** `(batch_size, seq_len)`\n",
        "\n",
        "    - **Note on Padding:** The model is NOT required to mask/zero-out\n",
        "      logits at padding positions. The training loop (`main_train.py`)\n",
        "      applies a strict mask to the loss function based on sequence\n",
        "      lengths. Therefore, it is acceptable for the model to output\n",
        "      noise at padding indices (common in unmasked Mamba/RNNs),\n",
        "      provided the causal history of *valid* tokens remains intact.\n",
        "\n",
        "--- [3. `get_scores_for_batch` Method] ---\n",
        "\n",
        "This is the adapter method called by `main_train.py`. It is\n",
        "responsible for unpacking the batch, calling its own `forward`\n",
        "method, and returning *all* per-token scores for the loss\n",
        "calculation.\n",
        "\n",
        "    - **Input:** `batch` (The raw, collated batch from the DataLoader.\n",
        "      Typically `[inputs, lengths, classification_targets]`)\n",
        "\n",
        "    - **Returns:** A tuple of `(scores, targets)`\n",
        "        - `scores`: `torch.Tensor` of shape `(batch_size, seq_len)`\n",
        "          (The per-token logits from the `forward` pass).\n",
        "        - `targets`: `torch.Tensor` of shape `(batch_size,)`\n",
        "          (The true class labels, e.g., [0, 1, 1, 0]).\n",
        "    *** IMPORTANT ***\n",
        "    Both `scores` and `targets` MUST be returned on the same\n",
        "    DEVICE (GPU/CPU) as the model. The training loop assumes\n",
        "    this adapter handles all device movement.\n",
        "\n",
        "--- [4. `get_final_scores` Method] ---\n",
        "\n",
        "This is the adapter method called by `evaluate.py`. It is\n",
        "responsible for unpacking the batch, calling `forward`, and\n",
        "returning the logit from *only* the single, final, unpadded token.\n",
        "\n",
        "    - **Input:** `batch` (The raw, collated batch, same as above).\n",
        "\n",
        "    - **Returns:** `last_logits`\n",
        "        - `last_logits`: `torch.Tensor` of shape `(batch_size,)`\n",
        "          (The logit from the last *real* token for each item\n",
        "          in the batch).\n",
        "\"\"\"\n",
        "def comment_only():\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a67a5fc-0de3-49d1-a0dd-424dbff15a28",
      "metadata": {
        "id": "8a67a5fc-0de3-49d1-a0dd-424dbff15a28"
      },
      "source": [
        "### LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3e0631-6cf8-4b17-83e2-7ad716992e30",
      "metadata": {
        "id": "5b3e0631-6cf8-4b17-83e2-7ad716992e30"
      },
      "outputs": [],
      "source": [
        "# --- Model Architecture ---\n",
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab_size, pad_id):\n",
        "        \"\"\"\n",
        "        Initializes the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            args: The full ArgumentParser namespace. Reads\n",
        "                  `args.lstm_hidden_dim` and `args.lstm_num_layers`.\n",
        "            vocab_size: The total vocabulary size for the embedding layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # --- CRITICAL CHANGE ---\n",
        "        # Using `vocab_size` (e.g., 32000) is robust and correct.\n",
        "        # Using `tokenizer_pad_id + 1` was brittle and would fail\n",
        "        # with many tokenizers where the pad ID is not the highest ID.\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=args.lstm_hidden_dim,\n",
        "            padding_idx=pad_id  # Use the REAL pad ID\n",
        "        )\n",
        "        # --- End of Change ---\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            args.lstm_hidden_dim,\n",
        "            args.lstm_hidden_dim,\n",
        "            num_layers=args.lstm_num_layers,\n",
        "            bidirectional=False,\n",
        "            dropout=0.5,\n",
        "            batch_first=True # Makes the permute/transpose logic simpler\n",
        "        )\n",
        "        self.out_linear = nn.Linear(args.lstm_hidden_dim, 1)\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        \"\"\"\n",
        "        Internal forward pass for the LSTM.\n",
        "        Requires `lengths` for sequence packing.\n",
        "        \"\"\"\n",
        "        # (batch_size, seq_len, hidden_dim)\n",
        "        embedded_inputs = self.embed(inputs)\n",
        "\n",
        "        # Pack sequence for efficient RNN processing\n",
        "        packed_inputs = pack_padded_sequence(\n",
        "            embedded_inputs,\n",
        "            lengths.cpu(), # Must be on CPU\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # rnn_output is (packed_batch, hidden_dim)\n",
        "        rnn_output, _ = self.rnn(packed_inputs)\n",
        "\n",
        "        # Unpack: (batch_size, seq_len, hidden_dim)\n",
        "        rnn_output, _ = pad_packed_sequence(\n",
        "            rnn_output,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # (batch_size, seq_len)\n",
        "        return self.out_linear(rnn_output).squeeze(2)\n",
        "\n",
        "    # ---\n",
        "    # --- Adapter Methods (The \"Contract\") ---\n",
        "    # ---\n",
        "\n",
        "    def get_scores_for_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for training.\n",
        "        Unpacks batch, calls `self.forward`, and returns all scores.\n",
        "        \"\"\"\n",
        "        # Unpack the batch as needed *by this model*\n",
        "        inputs, lengths, classification_targets = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "        classification_targets = classification_targets.to(self.embed.weight.device)\n",
        "\n",
        "        # Call this model's specific forward pass\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Return what the training loop needs\n",
        "        return scores, classification_targets\n",
        "\n",
        "    def get_final_scores(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for evaluation.\n",
        "        Unpacks batch, calls `self.forward`, and returns final logit.\n",
        "        \"\"\"\n",
        "        # We need all 3 components from the batch\n",
        "        inputs, lengths, _ = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "\n",
        "        # Call this model's specific forward pass\n",
        "        # scores shape: (batch_size, seq_len)\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Find the index of the last token\n",
        "        # Shape: (batch_size,)\n",
        "        last_indices = (lengths - 1).long()\n",
        "\n",
        "        # Gather the specific scores from those last indices\n",
        "        # Shape: (batch_size, 1) -> (batch_size,)\n",
        "        last_logits = scores.gather(\n",
        "            1, last_indices.unsqueeze(1)\n",
        "        ).squeeze(1)\n",
        "\n",
        "        return last_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mamba Model"
      ],
      "metadata": {
        "id": "-ICaBAN-C03a"
      },
      "id": "-ICaBAN-C03a"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "========================================================================\n",
        "Mamba Classifier Model Definition\n",
        "========================================================================\n",
        "\n",
        "This file implements a Mamba-based classifier that follows the same\n",
        "contract as the LSTM classifier for compatibility with the project's\n",
        "main training (`main_train.py`) and evaluation (`evaluate.py`) scripts.\n",
        "\n",
        "The Mamba architecture uses selective state space models (SSMs) for\n",
        "efficient sequence modeling with linear complexity in sequence length.\n",
        "\n",
        "Requirements:\n",
        "- mamba-ssm (install with: pip install mamba-ssm)\n",
        "- torch\n",
        "- einops\n",
        "\"\"\"\n",
        "\n",
        "class MambaClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab_size, pad_id):\n",
        "        \"\"\"\n",
        "        Initializes the Mamba model.\n",
        "\n",
        "        Args:\n",
        "            args: The full ArgumentParser namespace. Reads:\n",
        "                  - `args.mamba_d_model` (hidden dimension, default 256)\n",
        "                  - `args.mamba_d_state` (SSM state dimension, default 16)\n",
        "                  - `args.mamba_d_conv` (local convolution width, default 4)\n",
        "                  - `args.mamba_expand` (expansion factor, default 2)\n",
        "                  - `args.mamba_num_layers` (number of Mamba blocks, default 4)\n",
        "                  - `args.mamba_dropout` (dropout rate, default 0.1)\n",
        "            vocab_size: The total vocabulary size for the embedding layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Get hyperparameters from args with defaults\n",
        "        self.d_model = getattr(args, 'mamba_d_model', 256)\n",
        "        self.d_state = getattr(args, 'mamba_d_state', 16)\n",
        "        self.d_conv = getattr(args, 'mamba_d_conv', 4)\n",
        "        self.expand = getattr(args, 'mamba_expand', 2)\n",
        "        self.num_layers = getattr(args, 'mamba_num_layers', 4)\n",
        "        self.dropout_rate = getattr(args, 'mamba_dropout', 0.1)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.d_model,\n",
        "            padding_idx=pad_id  # Use the REAL pad ID\n",
        "        )\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "        # Stack of Mamba blocks\n",
        "        self.mamba_blocks = nn.ModuleList([\n",
        "            Mamba(\n",
        "                d_model=self.d_model,    # Model dimension\n",
        "                d_state=self.d_state,    # SSM state expansion factor\n",
        "                d_conv=self.d_conv,      # Local convolution width\n",
        "                expand=self.expand,      # Block expansion factor\n",
        "            )\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization between blocks\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(self.d_model)\n",
        "            for _ in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm before output\n",
        "        self.final_norm = nn.LayerNorm(self.d_model)\n",
        "\n",
        "        # Output projection to single logit per token\n",
        "        self.out_linear = nn.Linear(self.d_model, 1)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights with sensible defaults.\"\"\"\n",
        "        # Initialize embeddings\n",
        "        nn.init.normal_(self.embed.weight, mean=0.0, std=self.d_model ** -0.5)\n",
        "        if self.embed.padding_idx is not None:\n",
        "            nn.init.constant_(self.embed.weight[self.embed.padding_idx], 0)\n",
        "\n",
        "        # Initialize output projection\n",
        "        nn.init.xavier_uniform_(self.out_linear.weight)\n",
        "        nn.init.constant_(self.out_linear.bias, 0)\n",
        "\n",
        "    def forward(self, inputs, lengths=None):\n",
        "        \"\"\"\n",
        "        Optimized Mamba forward pass.\n",
        "        We do NOT mask internally. We allow Mamba to process padding tokens\n",
        "        naturally to preserve state dynamics. The training loop handles\n",
        "        ignoring the resulting padding outputs.\n",
        "        \"\"\"\n",
        "        # Embed tokens: (batch_size, seq_len, d_model)\n",
        "        x = self.embed(inputs)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # CRITICAL: Mamba kernels require contiguous memory layout\n",
        "        x = x.contiguous()\n",
        "\n",
        "        # Process through Mamba blocks\n",
        "        for mamba_block, layer_norm in zip(self.mamba_blocks, self.layer_norms):\n",
        "            # Pre-norm architecture\n",
        "            residual = x\n",
        "            x = layer_norm(x)\n",
        "\n",
        "            # Block\n",
        "            x = mamba_block(x)\n",
        "\n",
        "            # Residual connection\n",
        "            # NOTE: We removed \"if mask is not None: x = x * mask\"\n",
        "            # to allow correct State Space Model evolution.\n",
        "            x = residual + self.dropout(x)\n",
        "\n",
        "        # Final normalization\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Project to logits: (batch_size, seq_len)\n",
        "        scores = self.out_linear(x).squeeze(-1)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    # ---\n",
        "    # --- Adapter Methods (The \"Contract\") ---\n",
        "    # ---\n",
        "\n",
        "    def get_scores_for_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for training.\n",
        "        Unpacks batch, calls `self.forward`, and returns all scores.\n",
        "\n",
        "        Args:\n",
        "            batch: The raw, collated batch from the DataLoader.\n",
        "                   Typically [inputs, lengths, classification_targets]\n",
        "\n",
        "        Returns:\n",
        "            scores: torch.Tensor of shape (batch_size, seq_len)\n",
        "            targets: torch.Tensor of shape (batch_size,)\n",
        "        \"\"\"\n",
        "        # Unpack the batch\n",
        "        inputs, lengths, classification_targets = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "        classification_targets = classification_targets.to(self.embed.weight.device)\n",
        "\n",
        "        # Call forward pass\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Return scores and targets\n",
        "        return scores, classification_targets\n",
        "\n",
        "    def get_final_scores(self, batch):\n",
        "        \"\"\"\n",
        "        Adapter for evaluation.\n",
        "        Unpacks batch, calls `self.forward`, and returns final logit.\n",
        "\n",
        "        Args:\n",
        "            batch: The raw, collated batch from the DataLoader.\n",
        "\n",
        "        Returns:\n",
        "            last_logits: torch.Tensor of shape (batch_size,)\n",
        "                        The logit from the last real token for each item.\n",
        "        \"\"\"\n",
        "        # Unpack the batch\n",
        "        inputs, lengths, _ = batch\n",
        "\n",
        "        # Move tensors to the model's device\n",
        "        inputs = inputs.to(self.embed.weight.device)\n",
        "        lengths = lengths.to(self.embed.weight.device)\n",
        "\n",
        "        # Call forward pass\n",
        "        # scores shape: (batch_size, seq_len)\n",
        "        scores = self.forward(inputs, lengths)\n",
        "\n",
        "        # Find the index of the last token for each sequence\n",
        "        # Shape: (batch_size,)\n",
        "        last_indices = (lengths - 1).long()\n",
        "\n",
        "        # Gather the scores from the last valid position\n",
        "        # Shape: (batch_size, 1) -> (batch_size,)\n",
        "        last_logits = scores.gather(\n",
        "            1, last_indices.unsqueeze(1)\n",
        "        ).squeeze(1)\n",
        "\n",
        "        return last_logits"
      ],
      "metadata": {
        "id": "dZ5EBMa8C3hL"
      },
      "id": "dZ5EBMa8C3hL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f7044d4c-88b7-448d-95f5-da398cda5551",
      "metadata": {
        "id": "f7044d4c-88b7-448d-95f5-da398cda5551"
      },
      "source": [
        "### model init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbbd131-1784-448f-a7c7-1a47259ae942",
      "metadata": {
        "id": "9bbbd131-1784-448f-a7c7-1a47259ae942"
      },
      "outputs": [],
      "source": [
        "def get_model(args, vocab_size, pad_id):\n",
        "    \"\"\"\n",
        "    This factory function reads the --model_type argument\n",
        "    and returns the correct, initialized model.\n",
        "    \"\"\"\n",
        "    if args.model_type == 'lstm':\n",
        "        return LSTMClassifier(args, vocab_size, pad_id)\n",
        "    elif args.model_type == 'mamba':\n",
        "        return MambaClassifier(args, vocab_size, pad_id)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {args.model_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77d7f6d-8777-4c0c-af44-b0e2c6db4753",
      "metadata": {
        "id": "b77d7f6d-8777-4c0c-af44-b0e2c6db4753"
      },
      "source": [
        "## main_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5d3826-c3d9-4a3e-a7b5-e7688ce3254c",
      "metadata": {
        "id": "fc5d3826-c3d9-4a3e-a7b5-e7688ce3254c"
      },
      "outputs": [],
      "source": [
        "def train(model, dataset, optimizer, criterion, epoch, args):\n",
        "    model.train()\n",
        "    dataset.shuffle('train', seed=epoch + args.seed)\n",
        "\n",
        "    loader = dataset.loader('train', args.batch_size, num_workers=args.num_workers)\n",
        "    loss_meter = AverageMeter('loss', ':6.4f')\n",
        "    total_length = len(loader)\n",
        "    progress = ProgressMeter(total_length, [loss_meter], prefix='Training: ')\n",
        "\n",
        "    for batch_num, batch in enumerate(tqdm(loader, total=len(loader))):\n",
        "        # 1. Use the Contract to get scores and targets\n",
        "        # (The model handles device movement and internal unpacking)\n",
        "        scores, classification_targets = model.get_scores_for_batch(batch)\n",
        "\n",
        "        # 2. We still need 'lengths' for the mask logic below\n",
        "        _, lengths, _ = batch\n",
        "        lengths = lengths.to(scores.device) # Ensure it matches the GPU/CPU of scores\n",
        "\n",
        "        # --- This is the \"Implicit Prefix\" Loss Logic ---\n",
        "\n",
        "        expanded_labels = classification_targets.unsqueeze(1).expand(-1, scores.shape[1])\n",
        "\n",
        "        # 2. Get padding mask (batch_size, seq_len)\n",
        "        length_mask = pad_mask(lengths) # 1 for real tokens, 0 for padding\n",
        "\n",
        "        # 3. Flatten scores, labels, and mask\n",
        "        scores_flat = scores.flatten()\n",
        "        labels_flat = expanded_labels.flatten().float()\n",
        "        mask_flat = length_mask.flatten()\n",
        "\n",
        "        # 4. Select only the non-padded tokens for loss calculation\n",
        "        scores_unpadded = scores_flat[mask_flat == 1]\n",
        "        labels_unpadded = labels_flat[mask_flat == 1]\n",
        "\n",
        "        # 5. Calculate loss\n",
        "        loss = criterion(scores_unpadded, labels_unpadded)\n",
        "        # --- End of Implicit Logic ---\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # --- SANITY CHECK ---\n",
        "        if dataset.tokenizer_pad_id is not None:\n",
        "            # Assuming the model has an 'embed' layer.\n",
        "            # If you change model types (e.g. to Mamba), ensure this attribute path exists.\n",
        "            pad_grad = model.embed.weight.grad[dataset.tokenizer_pad_id]\n",
        "\n",
        "            # It's possible pad_grad is None if the batch had NO padding (unlikely but possible)\n",
        "            if pad_grad is not None:\n",
        "                if torch.sum(torch.abs(pad_grad)) > 0:\n",
        "                    raise RuntimeError(f\"FATAL: Model is learning from PAD token {dataset.tokenizer_pad_id}!\")\n",
        "        # --------------------\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_meter.update(loss.item(), scores.size(0))\n",
        "        if batch_num % args.print_freq == 0:\n",
        "            progress.display(batch_num)\n",
        "\n",
        "    progress.display(total_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a22a959a-4ca4-4fdc-8eda-59b9eaae04ff",
      "metadata": {
        "id": "a22a959a-4ca4-4fdc-8eda-59b9eaae04ff"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataset, criterion, args):\n",
        "    model.eval()\n",
        "    loader = dataset.loader('val', args.batch_size, num_workers=args.num_workers)\n",
        "\n",
        "    loss_meter = AverageMeter('loss', ':6.4f')\n",
        "    acc_meter = AverageMeter('acc', ':6.2f') # Add an accuracy meter\n",
        "\n",
        "    total_length = len(loader)\n",
        "    # Add acc_meter to the display list\n",
        "    progress = ProgressMeter(total_length, [loss_meter, acc_meter], prefix='Validation: ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, batch in enumerate(tqdm(loader, total=len(loader))):\n",
        "\n",
        "            # 1. Use the Contract (Handles device movement & unpacking)\n",
        "            scores, classification_targets = model.get_scores_for_batch(batch)\n",
        "\n",
        "            # 2. Get lengths specifically for masking (Manual step)\n",
        "            _, lengths, _ = batch\n",
        "            lengths = lengths.to(scores.device)\n",
        "\n",
        "            # --- Identical Implicit Loss Logic ---\n",
        "            expanded_labels = classification_targets.unsqueeze(1).expand(-1, scores.shape[1])\n",
        "            length_mask = pad_mask(lengths)\n",
        "\n",
        "            scores_flat = scores.flatten()\n",
        "            labels_flat = expanded_labels.flatten().float()\n",
        "            mask_flat = length_mask.flatten()\n",
        "\n",
        "            scores_unpadded = scores_flat[mask_flat == 1]\n",
        "            labels_unpadded = labels_flat[mask_flat == 1]\n",
        "\n",
        "            if scores_unpadded.nelement() > 0:\n",
        "                # Calculate Loss\n",
        "                loss = criterion(scores_unpadded, labels_unpadded)\n",
        "\n",
        "                # --- Calculate Accuracy ---\n",
        "                # Sigmoid converts logits to probs (0 to 1)\n",
        "                probs = torch.sigmoid(scores_unpadded)\n",
        "                # Anything > 0.5 is a prediction for Class 1 (EB11)\n",
        "                preds = (probs > 0.5).float()\n",
        "                # Compare preds to true labels\n",
        "                acc = (preds == labels_unpadded).float().mean()\n",
        "                # --------------------------\n",
        "\n",
        "                # Update meters (Use scores.size(0) for batch size)\n",
        "                loss_meter.update(loss.item(), scores.size(0))\n",
        "                acc_meter.update(acc.item(), scores.size(0))\n",
        "\n",
        "    progress.display(total_length)\n",
        "\n",
        "    # Return loss (or accuracy, depending on what you want to track for early stopping)\n",
        "    return loss_meter.avg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dry_run(args, model, dataset):\n",
        "    print(\"--- INITIATING DRY RUN ---\")\n",
        "\n",
        "    # Ensure model is in training mode so gradients are generated\n",
        "    model.train()\n",
        "\n",
        "    # 1. Get one batch\n",
        "    loader = dataset.loader('train', batch_size=2)\n",
        "    batch = next(iter(loader))\n",
        "\n",
        "    # 2. Forward Pass\n",
        "    # Your model's 'get_scores_for_batch' handles moving tensors to GPU,\n",
        "    # so we don't need to manually .to(device) the batch here.\n",
        "    scores, targets = model.get_scores_for_batch(batch)\n",
        "\n",
        "    # 3. Backward Pass (Dummy Loss)\n",
        "    # We just need a scalar to test that backprop works without OOMing.\n",
        "    # We don't need the real loss function here.\n",
        "    loss = scores.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    # 4. Check Max ID vs Vocab Size (The critical check for your bug)\n",
        "    input_ids = batch[0]\n",
        "    # Access the embedding weight size directly\n",
        "    vocab_limit = model.embed.weight.size(0)\n",
        "    if input_ids.max() >= vocab_limit:\n",
        "        raise ValueError(f\"CRITICAL: Batch contains Token ID {input_ids.max()}, but Embedding size is only {vocab_limit}. Adjust get_model()!\")\n",
        "\n",
        "    # 5. Save/Load Check (Verifies serialization)\n",
        "    torch.save(model.state_dict(), \"dry_run.pth\")\n",
        "    model.load_state_dict(torch.load(\"dry_run.pth\"))\n",
        "\n",
        "    print(\"--- DRY RUN PASSED ---\")\n",
        "\n",
        "    # Clean up\n",
        "    if os.path.exists(\"dry_run.pth\"):\n",
        "        os.remove(\"dry_run.pth\")\n",
        "\n",
        "    # Zero gradients so this test doesn't affect the first real training step\n",
        "    model.zero_grad()\n"
      ],
      "metadata": {
        "id": "zBAwDxudIEqt"
      },
      "id": "zBAwDxudIEqt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test_set(model, dataset, args, checkpoint_path):\n",
        "    print(f\"\\n--- STARTING FINAL EVALUATION ---\")\n",
        "    print(f\"Loading best checkpoint from: {checkpoint_path}\")\n",
        "\n",
        "    # Load the best weights\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    model.eval()\n",
        "    loader = dataset.loader('test', args.batch_size, num_workers=args.num_workers)\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    print(\"Running inference on Test Set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, total=len(loader)):\n",
        "            # 1. Get the logits for the LAST token in the sequence\n",
        "            # We use get_final_scores because we care about the document-level decision,\n",
        "            # not the 'loss at every step' intermediate predictions.\n",
        "            logits = model.get_final_scores(batch)\n",
        "\n",
        "            # 2. Convert logits to probabilities and then to binary predictions\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).long()\n",
        "\n",
        "            # 3. Get actual labels (Index 2 in your collate tuple)\n",
        "            # (inputs, lengths, classification_labels)\n",
        "            targets = batch[2]\n",
        "\n",
        "            # 4. Move to CPU and collect\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    # --- Calculate Metrics ---\n",
        "    acc = accuracy_score(all_targets, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='binary')\n",
        "    conf_mat = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"TEST SET RESULTS\")\n",
        "    print(\"=\"*30)\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_mat)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Detailed report useful for seeing class imbalances\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_targets, all_preds, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    return f1"
      ],
      "metadata": {
        "id": "v-rxxs3cPoty"
      },
      "id": "v-rxxs3cPoty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    # 1. Set the python built-in random seed\n",
        "    random.seed(seed)\n",
        "\n",
        "    # 2. Set the numpy seed\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 3. Set the pytorch seed\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # If using multi-GPU\n",
        "\n",
        "    # 4. Important: Force CuDNN to be deterministic\n",
        "    # This slows down training slightly but ensures 'exact' reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 5. Set hashing seed (vital for dictionary ordering/hashing)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    print(f\"Global seed set to {seed}\")"
      ],
      "metadata": {
        "id": "IGJwmkvWKqBb"
      },
      "id": "IGJwmkvWKqBb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c252e0-752d-4389-90fc-1a37ec0f1f1b",
      "metadata": {
        "id": "40c252e0-752d-4389-90fc-1a37ec0f1f1b"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    seed_everything(SEED)\n",
        "\n",
        "    args = setup()\n",
        "    login(token=args.hf_token)\n",
        "\n",
        "    # Hard-code the task\n",
        "    args.task = 'transfer'\n",
        "    args.device = torch.device(DEVICE)\n",
        "\n",
        "    dataset = Dataset(args)\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "\n",
        "    model = get_model(args, len(dataset.tokenizer), dataset.tokenizer_pad_id)\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # --- ADDED: Scheduler ---\n",
        "    # If validation loss doesn't improve for 2 epochs, cut LR by half\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer, mode='min', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    best_val_metric = 1e8\n",
        "\n",
        "    print('Model Parameters:', num_params(model))\n",
        "    criterion = nn.BCEWithLogitsLoss().to(args.device)\n",
        "\n",
        "    dry_run(args, model, dataset)\n",
        "\n",
        "    now = datetime.now()\n",
        "    date_string = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    save_path = os.path.join(args.save_dir, f'{args.model_type}_{date_string}.pth.tar')\n",
        "\n",
        "    # --- ADDED: Early Stopping Config ---\n",
        "    early_stopping_patience = 5\n",
        "    epochs_no_improve = 0\n",
        "    # ------------------------------------\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"--- TRAINING: Epoch {epoch} at {time.ctime()} ---\")\n",
        "        train(model, dataset, optimizer, criterion, epoch, args)\n",
        "\n",
        "        print(f\"--- VALIDATION: Epoch {epoch} at {time.ctime()} ---\")\n",
        "        metric = validate(model, dataset, criterion, args)\n",
        "\n",
        "        # Update the scheduler based on validation loss\n",
        "        scheduler.step(metric)\n",
        "\n",
        "        # --- ADD THIS TO REPLACE VERBOSE=TRUE ---\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Current Learning Rate: {current_lr}\")\n",
        "        # ----------------------------------------\n",
        "\n",
        "        if metric < best_val_metric:\n",
        "            print(f'New best val metric: {metric:.4f}')\n",
        "            best_val_metric = metric\n",
        "\n",
        "            # Reset patience because we improved\n",
        "            epochs_no_improve = 0\n",
        "\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_metric': best_val_metric,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'args': args\n",
        "            }, save_path)\n",
        "        else:\n",
        "            # We didn't improve\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"No improvement for {epochs_no_improve} epochs.\")\n",
        "\n",
        "            if epochs_no_improve >= early_stopping_patience:\n",
        "                print(f\"Early stopping triggered after epoch {epoch}!\")\n",
        "                print(f'Final val metric: {best_val_metric:.4f}')\n",
        "                break\n",
        "    evaluate_test_set(model, dataset, args, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4484f0c6-ddce-40c1-a3e9-7ad98e47a01e",
      "metadata": {
        "id": "4484f0c6-ddce-40c1-a3e9-7ad98e47a01e"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}